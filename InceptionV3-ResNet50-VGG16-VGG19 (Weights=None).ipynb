{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D, UpSampling2D, AveragePooling2D,ZeroPadding2D,Convolution2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "from IPython.display import SVG, Image\n",
    "import cv2\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2785 images belonging to 2 classes.\n",
      "Found 347 images belonging to 2 classes.\n",
      "Found 348 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_size=(100,100)\n",
    "batch_size = 32\n",
    "datagen_train=ImageDataGenerator(horizontal_flip=True)\n",
    "train_generator=datagen_train.flow_from_directory(\"EyeTrack_Augmented/training/EyeTrack_Augmented\",\n",
    "target_size=img_size,\n",
    "batch_size=batch_size,\n",
    "class_mode='categorical',\n",
    "color_mode =\"grayscale\",\n",
    "shuffle=True)\n",
    "\n",
    "datagen_validation=ImageDataGenerator(horizontal_flip=True)\n",
    "validation_generator=datagen_validation.flow_from_directory(\"EyeTrack_Augmented/validation/EyeTrack_Augmented\",\n",
    "target_size=img_size,\n",
    "batch_size=batch_size,\n",
    "class_mode='categorical',\n",
    "color_mode =\"grayscale\",\n",
    "shuffle=True)\n",
    "\n",
    "datagen_test=ImageDataGenerator(horizontal_flip=True)\n",
    "test_generator=datagen_test.flow_from_directory(\"EyeTrack_Augmented/testing/EyeTrack_Augmented\",\n",
    "target_size=img_size,\n",
    "batch_size=batch_size,\n",
    "class_mode='categorical',\n",
    "color_mode =\"grayscale\",\n",
    "shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inception = InceptionV3(input_shape=[100,100]+ [1], weights=None, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "for layer in inception.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(inception.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=inception.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 100, 100, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_282 (Conv2D)             (None, 49, 49, 32)   288         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 49, 49, 32)   96          conv2d_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, 49, 49, 32)   0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_283 (Conv2D)             (None, 47, 47, 32)   9216        activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 47, 47, 32)   96          conv2d_283[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, 47, 47, 32)   0           batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_284 (Conv2D)             (None, 47, 47, 64)   18432       activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 47, 47, 64)   192         conv2d_284[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, 47, 47, 64)   0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 23, 23, 64)   0           activation_284[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_285 (Conv2D)             (None, 23, 23, 80)   5120        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 23, 23, 80)   240         conv2d_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, 23, 23, 80)   0           batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_286 (Conv2D)             (None, 21, 21, 192)  138240      activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 21, 21, 192)  576         conv2d_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_286 (Activation)     (None, 21, 21, 192)  0           batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 10, 10, 192)  0           activation_286[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_290 (Conv2D)             (None, 10, 10, 64)   12288       max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchN (None, 10, 10, 64)   192         conv2d_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_290 (Activation)     (None, 10, 10, 64)   0           batch_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_288 (Conv2D)             (None, 10, 10, 48)   9216        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_291 (Conv2D)             (None, 10, 10, 96)   55296       activation_290[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 10, 10, 48)   144         conv2d_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchN (None, 10, 10, 96)   288         conv2d_291[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, 10, 10, 48)   0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_291 (Activation)     (None, 10, 10, 96)   0           batch_normalization_291[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_27 (AveragePo (None, 10, 10, 192)  0           max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_287 (Conv2D)             (None, 10, 10, 64)   12288       max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_289 (Conv2D)             (None, 10, 10, 64)   76800       activation_288[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_292 (Conv2D)             (None, 10, 10, 96)   82944       activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_293 (Conv2D)             (None, 10, 10, 32)   6144        average_pooling2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 10, 10, 64)   192         conv2d_287[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchN (None, 10, 10, 64)   192         conv2d_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 10, 10, 96)   288         conv2d_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 10, 10, 32)   96          conv2d_293[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_287 (Activation)     (None, 10, 10, 64)   0           batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, 10, 10, 64)   0           batch_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, 10, 10, 96)   0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, 10, 10, 32)   0           batch_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 10, 10, 256)  0           activation_287[0][0]             \n",
      "                                                                 activation_289[0][0]             \n",
      "                                                                 activation_292[0][0]             \n",
      "                                                                 activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_297 (Conv2D)             (None, 10, 10, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_297 (BatchN (None, 10, 10, 64)   192         conv2d_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, 10, 10, 64)   0           batch_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_295 (Conv2D)             (None, 10, 10, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_298 (Conv2D)             (None, 10, 10, 96)   55296       activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, 10, 10, 48)   144         conv2d_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, 10, 10, 96)   288         conv2d_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_295 (Activation)     (None, 10, 10, 48)   0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_298 (Activation)     (None, 10, 10, 96)   0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_28 (AveragePo (None, 10, 10, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_294 (Conv2D)             (None, 10, 10, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_296 (Conv2D)             (None, 10, 10, 64)   76800       activation_295[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_299 (Conv2D)             (None, 10, 10, 96)   82944       activation_298[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_300 (Conv2D)             (None, 10, 10, 64)   16384       average_pooling2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, 10, 10, 64)   192         conv2d_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, 10, 10, 64)   192         conv2d_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_299 (BatchN (None, 10, 10, 96)   288         conv2d_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, 10, 10, 64)   192         conv2d_300[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, 10, 10, 64)   0           batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, 10, 10, 64)   0           batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 10, 10, 96)   0           batch_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 10, 10, 64)   0           batch_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 10, 10, 288)  0           activation_294[0][0]             \n",
      "                                                                 activation_296[0][0]             \n",
      "                                                                 activation_299[0][0]             \n",
      "                                                                 activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_304 (Conv2D)             (None, 10, 10, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_304 (BatchN (None, 10, 10, 64)   192         conv2d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 10, 10, 64)   0           batch_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_302 (Conv2D)             (None, 10, 10, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_305 (Conv2D)             (None, 10, 10, 96)   55296       activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_302 (BatchN (None, 10, 10, 48)   144         conv2d_302[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, 10, 10, 96)   288         conv2d_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 10, 10, 48)   0           batch_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 10, 10, 96)   0           batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_29 (AveragePo (None, 10, 10, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_301 (Conv2D)             (None, 10, 10, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_303 (Conv2D)             (None, 10, 10, 64)   76800       activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_306 (Conv2D)             (None, 10, 10, 96)   82944       activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_307 (Conv2D)             (None, 10, 10, 64)   18432       average_pooling2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_301 (BatchN (None, 10, 10, 64)   192         conv2d_301[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_303 (BatchN (None, 10, 10, 64)   192         conv2d_303[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 10, 10, 96)   288         conv2d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 10, 10, 64)   192         conv2d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 10, 10, 64)   0           batch_normalization_301[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 10, 10, 64)   0           batch_normalization_303[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 10, 10, 96)   0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 10, 10, 64)   0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 10, 10, 288)  0           activation_301[0][0]             \n",
      "                                                                 activation_303[0][0]             \n",
      "                                                                 activation_306[0][0]             \n",
      "                                                                 activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_309 (Conv2D)             (None, 10, 10, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 10, 10, 64)   192         conv2d_309[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 10, 10, 64)   0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_310 (Conv2D)             (None, 10, 10, 96)   55296       activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 10, 10, 96)   288         conv2d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 10, 10, 96)   0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_308 (Conv2D)             (None, 4, 4, 384)    995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_311 (Conv2D)             (None, 4, 4, 96)     82944       activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 4, 4, 384)    1152        conv2d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 4, 4, 96)     288         conv2d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 4, 4, 384)    0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 4, 4, 96)     0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 4, 4, 288)    0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 4, 4, 768)    0           activation_308[0][0]             \n",
      "                                                                 activation_311[0][0]             \n",
      "                                                                 max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 4, 4, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 4, 4, 128)    384         conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 4, 4, 128)    0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 4, 4, 128)    114688      activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 4, 4, 128)    384         conv2d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 4, 4, 128)    0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_313 (Conv2D)             (None, 4, 4, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 4, 4, 128)    114688      activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 4, 4, 128)    384         conv2d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 4, 4, 128)    384         conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 4, 4, 128)    0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 4, 4, 128)    0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_314 (Conv2D)             (None, 4, 4, 128)    114688      activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 4, 4, 128)    114688      activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 4, 4, 128)    384         conv2d_314[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 4, 4, 128)    384         conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 4, 4, 128)    0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 4, 4, 128)    0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_30 (AveragePo (None, 4, 4, 768)    0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_312 (Conv2D)             (None, 4, 4, 192)    147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 4, 4, 192)    172032      activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 4, 4, 192)    172032      activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_321 (Conv2D)             (None, 4, 4, 192)    147456      average_pooling2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 4, 4, 192)    576         conv2d_312[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 4, 4, 192)    576         conv2d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 4, 4, 192)    576         conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 4, 4, 192)    576         conv2d_321[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 4, 4, 192)    0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 4, 4, 192)    0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 4, 4, 192)    0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 4, 4, 192)    0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 4, 4, 768)    0           activation_312[0][0]             \n",
      "                                                                 activation_315[0][0]             \n",
      "                                                                 activation_320[0][0]             \n",
      "                                                                 activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 4, 4, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_326 (BatchN (None, 4, 4, 160)    480         conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 4, 4, 160)    0           batch_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 4, 4, 160)    179200      activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 4, 4, 160)    480         conv2d_327[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 4, 4, 160)    0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 4, 4, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 4, 4, 160)    179200      activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 4, 4, 160)    480         conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 4, 4, 160)    480         conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 4, 4, 160)    0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 4, 4, 160)    0           batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 4, 4, 160)    179200      activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 4, 4, 160)    179200      activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 4, 4, 160)    480         conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 4, 4, 160)    480         conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 4, 4, 160)    0           batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 4, 4, 160)    0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_31 (AveragePo (None, 4, 4, 768)    0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_322 (Conv2D)             (None, 4, 4, 192)    147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 4, 4, 192)    215040      activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 4, 4, 192)    215040      activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 4, 4, 192)    147456      average_pooling2d_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 4, 4, 192)    576         conv2d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 4, 4, 192)    576         conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 4, 4, 192)    576         conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 4, 4, 192)    576         conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 4, 4, 192)    0           batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 4, 4, 192)    0           batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 4, 4, 192)    0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 4, 4, 192)    0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 4, 4, 768)    0           activation_322[0][0]             \n",
      "                                                                 activation_325[0][0]             \n",
      "                                                                 activation_330[0][0]             \n",
      "                                                                 activation_331[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 4, 4, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 4, 4, 160)    480         conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 4, 4, 160)    0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 4, 4, 160)    179200      activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 4, 4, 160)    480         conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 4, 4, 160)    0           batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 4, 4, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 4, 4, 160)    179200      activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 4, 4, 160)    480         conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_338 (BatchN (None, 4, 4, 160)    480         conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 4, 4, 160)    0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 4, 4, 160)    0           batch_normalization_338[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 4, 4, 160)    179200      activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 4, 4, 160)    179200      activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 4, 4, 160)    480         conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 4, 4, 160)    480         conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 4, 4, 160)    0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 4, 4, 160)    0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_32 (AveragePo (None, 4, 4, 768)    0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 4, 4, 192)    147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 4, 4, 192)    215040      activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 4, 4, 192)    215040      activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 4, 4, 192)    147456      average_pooling2d_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 4, 4, 192)    576         conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 4, 4, 192)    576         conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 4, 4, 192)    576         conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_341 (BatchN (None, 4, 4, 192)    576         conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 4, 4, 192)    0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 4, 4, 192)    0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 4, 4, 192)    0           batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 4, 4, 192)    0           batch_normalization_341[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 4, 4, 768)    0           activation_332[0][0]             \n",
      "                                                                 activation_335[0][0]             \n",
      "                                                                 activation_340[0][0]             \n",
      "                                                                 activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 4, 4, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 4, 4, 192)    576         conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 4, 4, 192)    0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 4, 4, 192)    258048      activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 4, 4, 192)    576         conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 4, 4, 192)    0           batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 4, 4, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 4, 4, 192)    258048      activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 4, 4, 192)    576         conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_348 (BatchN (None, 4, 4, 192)    576         conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 4, 4, 192)    0           batch_normalization_343[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 4, 4, 192)    0           batch_normalization_348[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 4, 4, 192)    258048      activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 4, 4, 192)    258048      activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 4, 4, 192)    576         conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 4, 4, 192)    576         conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 4, 4, 192)    0           batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 4, 4, 192)    0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_33 (AveragePo (None, 4, 4, 768)    0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 4, 4, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 4, 4, 192)    258048      activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 4, 4, 192)    258048      activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 4, 4, 192)    147456      average_pooling2d_33[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 4, 4, 192)    576         conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_345 (BatchN (None, 4, 4, 192)    576         conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 4, 4, 192)    576         conv2d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_351 (BatchN (None, 4, 4, 192)    576         conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 4, 4, 192)    0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 4, 4, 192)    0           batch_normalization_345[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 4, 4, 192)    0           batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_351 (Activation)     (None, 4, 4, 192)    0           batch_normalization_351[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 4, 4, 768)    0           activation_342[0][0]             \n",
      "                                                                 activation_345[0][0]             \n",
      "                                                                 activation_350[0][0]             \n",
      "                                                                 activation_351[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 4, 4, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_354 (BatchN (None, 4, 4, 192)    576         conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_354 (Activation)     (None, 4, 4, 192)    0           batch_normalization_354[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 4, 4, 192)    258048      activation_354[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_355 (BatchN (None, 4, 4, 192)    576         conv2d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_355 (Activation)     (None, 4, 4, 192)    0           batch_normalization_355[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 4, 4, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 4, 4, 192)    258048      activation_355[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_352 (BatchN (None, 4, 4, 192)    576         conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_356 (BatchN (None, 4, 4, 192)    576         conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_352 (Activation)     (None, 4, 4, 192)    0           batch_normalization_352[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_356 (Activation)     (None, 4, 4, 192)    0           batch_normalization_356[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 1, 1, 320)    552960      activation_352[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 1, 1, 192)    331776      activation_356[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_353 (BatchN (None, 1, 1, 320)    960         conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_357 (BatchN (None, 1, 1, 192)    576         conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_353 (Activation)     (None, 1, 1, 320)    0           batch_normalization_353[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_357 (Activation)     (None, 1, 1, 192)    0           batch_normalization_357[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 1, 1, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 1, 1, 1280)   0           activation_353[0][0]             \n",
      "                                                                 activation_357[0][0]             \n",
      "                                                                 max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 1, 1, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_362 (BatchN (None, 1, 1, 448)    1344        conv2d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_362 (Activation)     (None, 1, 1, 448)    0           batch_normalization_362[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 1, 1, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 1, 1, 384)    1548288     activation_362[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_359 (BatchN (None, 1, 1, 384)    1152        conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_363 (BatchN (None, 1, 1, 384)    1152        conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_359 (Activation)     (None, 1, 1, 384)    0           batch_normalization_359[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_363 (Activation)     (None, 1, 1, 384)    0           batch_normalization_363[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 1, 1, 384)    442368      activation_359[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 1, 1, 384)    442368      activation_359[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 1, 1, 384)    442368      activation_363[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 1, 1, 384)    442368      activation_363[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_34 (AveragePo (None, 1, 1, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 1, 1, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_360 (BatchN (None, 1, 1, 384)    1152        conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_361 (BatchN (None, 1, 1, 384)    1152        conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_364 (BatchN (None, 1, 1, 384)    1152        conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_365 (BatchN (None, 1, 1, 384)    1152        conv2d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 1, 1, 192)    245760      average_pooling2d_34[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_358 (BatchN (None, 1, 1, 320)    960         conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_360 (Activation)     (None, 1, 1, 384)    0           batch_normalization_360[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_361 (Activation)     (None, 1, 1, 384)    0           batch_normalization_361[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_364 (Activation)     (None, 1, 1, 384)    0           batch_normalization_364[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_365 (Activation)     (None, 1, 1, 384)    0           batch_normalization_365[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_366 (BatchN (None, 1, 1, 192)    576         conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_358 (Activation)     (None, 1, 1, 320)    0           batch_normalization_358[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 1, 1, 768)    0           activation_360[0][0]             \n",
      "                                                                 activation_361[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1, 1, 768)    0           activation_364[0][0]             \n",
      "                                                                 activation_365[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_366 (Activation)     (None, 1, 1, 192)    0           batch_normalization_366[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 1, 1, 2048)   0           activation_358[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_6[0][0]              \n",
      "                                                                 activation_366[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_371 (Conv2D)             (None, 1, 1, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_371 (BatchN (None, 1, 1, 448)    1344        conv2d_371[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_371 (Activation)     (None, 1, 1, 448)    0           batch_normalization_371[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 1, 1, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_372 (Conv2D)             (None, 1, 1, 384)    1548288     activation_371[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_368 (BatchN (None, 1, 1, 384)    1152        conv2d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_372 (BatchN (None, 1, 1, 384)    1152        conv2d_372[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_368 (Activation)     (None, 1, 1, 384)    0           batch_normalization_368[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_372 (Activation)     (None, 1, 1, 384)    0           batch_normalization_372[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_369 (Conv2D)             (None, 1, 1, 384)    442368      activation_368[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_370 (Conv2D)             (None, 1, 1, 384)    442368      activation_368[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_373 (Conv2D)             (None, 1, 1, 384)    442368      activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_374 (Conv2D)             (None, 1, 1, 384)    442368      activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_35 (AveragePo (None, 1, 1, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 1, 1, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_369 (BatchN (None, 1, 1, 384)    1152        conv2d_369[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_370 (BatchN (None, 1, 1, 384)    1152        conv2d_370[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_373 (BatchN (None, 1, 1, 384)    1152        conv2d_373[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_374 (BatchN (None, 1, 1, 384)    1152        conv2d_374[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_375 (Conv2D)             (None, 1, 1, 192)    393216      average_pooling2d_35[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_367 (BatchN (None, 1, 1, 320)    960         conv2d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_369 (Activation)     (None, 1, 1, 384)    0           batch_normalization_369[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_370 (Activation)     (None, 1, 1, 384)    0           batch_normalization_370[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_373 (Activation)     (None, 1, 1, 384)    0           batch_normalization_373[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_374 (Activation)     (None, 1, 1, 384)    0           batch_normalization_374[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_375 (BatchN (None, 1, 1, 192)    576         conv2d_375[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_367 (Activation)     (None, 1, 1, 320)    0           batch_normalization_367[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 1, 1, 768)    0           activation_369[0][0]             \n",
      "                                                                 activation_370[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1, 1, 768)    0           activation_373[0][0]             \n",
      "                                                                 activation_374[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_375 (Activation)     (None, 1, 1, 192)    0           batch_normalization_375[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 1, 1, 2048)   0           activation_367[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_7[0][0]              \n",
      "                                                                 activation_375[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            4098        flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,806,306\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 21,802,208\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "optimum= Adam(learning_rate=0.000005)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimum, metrics=['accuracy',recall,precision,tf.keras.metrics.AUC(curve=\"ROC\"),tf.keras.metrics.FalsePositives(),tf.keras.metrics.TruePositives(),tf.keras.metrics.FalseNegatives(),tf.keras.metrics.TrueNegatives(),tf.keras.metrics.SensitivityAtSpecificity(0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "87/87 [==============================] - 20s 125ms/step - loss: 0.6933 - accuracy: 0.6012 - recall: 0.6056 - precision: 0.6056 - auc_2: 0.3747 - false_positives_2: 1098.0000 - true_positives_2: 1655.0000 - false_negatives_2: 1098.0000 - true_negatives_2: 1655.0000 - sensitivity_at_specificity_2: 0.0425 - val_loss: 0.6937 - val_accuracy: 0.5562 - val_recall: 0.5562 - val_precision: 0.5562 - val_auc_2: 0.3610 - val_false_positives_2: 142.0000 - val_true_positives_2: 178.0000 - val_false_negatives_2: 142.0000 - val_true_negatives_2: 178.0000 - val_sensitivity_at_specificity_2: 0.0594\n",
      "Epoch 2/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.6932 - accuracy: 0.6008 - recall: 0.6052 - precision: 0.6052 - auc_2: 0.3817 - false_positives_2: 1099.0000 - true_positives_2: 1654.0000 - false_negatives_2: 1099.0000 - true_negatives_2: 1654.0000 - sensitivity_at_specificity_2: 0.0559 - val_loss: 0.6939 - val_accuracy: 0.5344 - val_recall: 0.5344 - val_precision: 0.5344 - val_auc_2: 0.3348 - val_false_positives_2: 149.0000 - val_true_positives_2: 171.0000 - val_false_negatives_2: 149.0000 - val_true_negatives_2: 171.0000 - val_sensitivity_at_specificity_2: 0.0469\n",
      "Epoch 3/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6931 - accuracy: 0.6015 - recall: 0.6060 - precision: 0.6060 - auc_2: 0.3905 - false_positives_2: 1097.0000 - true_positives_2: 1656.0000 - false_negatives_2: 1097.0000 - true_negatives_2: 1656.0000 - sensitivity_at_specificity_2: 0.0559 - val_loss: 0.6936 - val_accuracy: 0.5469 - val_recall: 0.5469 - val_precision: 0.5469 - val_auc_2: 0.3524 - val_false_positives_2: 145.0000 - val_true_positives_2: 175.0000 - val_false_negatives_2: 145.0000 - val_true_negatives_2: 175.0000 - val_sensitivity_at_specificity_2: 0.0531\n",
      "Epoch 4/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6931 - accuracy: 0.5997 - recall: 0.6042 - precision: 0.6042 - auc_2: 0.4023 - false_positives_2: 1102.0000 - true_positives_2: 1651.0000 - false_negatives_2: 1102.0000 - true_negatives_2: 1651.0000 - sensitivity_at_specificity_2: 0.0690 - val_loss: 0.6934 - val_accuracy: 0.5688 - val_recall: 0.5688 - val_precision: 0.5688 - val_auc_2: 0.3942 - val_false_positives_2: 138.0000 - val_true_positives_2: 182.0000 - val_false_negatives_2: 138.0000 - val_true_negatives_2: 182.0000 - val_sensitivity_at_specificity_2: 0.0625\n",
      "Epoch 5/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6930 - accuracy: 0.6023 - recall: 0.6067 - precision: 0.6067 - auc_2: 0.4136 - false_positives_2: 1095.0000 - true_positives_2: 1658.0000 - false_negatives_2: 1095.0000 - true_negatives_2: 1658.0000 - sensitivity_at_specificity_2: 0.0770 - val_loss: 0.6936 - val_accuracy: 0.5406 - val_recall: 0.5406 - val_precision: 0.5406 - val_auc_2: 0.3761 - val_false_positives_2: 147.0000 - val_true_positives_2: 173.0000 - val_false_negatives_2: 147.0000 - val_true_negatives_2: 173.0000 - val_sensitivity_at_specificity_2: 0.0656\n",
      "Epoch 6/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.6929 - accuracy: 0.6012 - recall: 0.6056 - precision: 0.6056 - auc_2: 0.4252 - false_positives_2: 1098.0000 - true_positives_2: 1655.0000 - false_negatives_2: 1098.0000 - true_negatives_2: 1655.0000 - sensitivity_at_specificity_2: 0.0934 - val_loss: 0.6934 - val_accuracy: 0.5562 - val_recall: 0.5562 - val_precision: 0.5562 - val_auc_2: 0.3886 - val_false_positives_2: 142.0000 - val_true_positives_2: 178.0000 - val_false_negatives_2: 142.0000 - val_true_negatives_2: 178.0000 - val_sensitivity_at_specificity_2: 0.0875\n",
      "Epoch 7/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6928 - accuracy: 0.6023 - recall: 0.5955 - precision: 0.5955 - auc_2: 0.4232 - false_positives_2: 1095.0000 - true_positives_2: 1658.0000 - false_negatives_2: 1095.0000 - true_negatives_2: 1658.0000 - sensitivity_at_specificity_2: 0.0697 - val_loss: 0.6932 - val_accuracy: 0.5656 - val_recall: 0.5656 - val_precision: 0.5656 - val_auc_2: 0.3938 - val_false_positives_2: 139.0000 - val_true_positives_2: 181.0000 - val_false_negatives_2: 139.0000 - val_true_negatives_2: 181.0000 - val_sensitivity_at_specificity_2: 0.0625\n",
      "Epoch 8/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6927 - accuracy: 0.6023 - recall: 0.6067 - precision: 0.6067 - auc_2: 0.4418 - false_positives_2: 1095.0000 - true_positives_2: 1658.0000 - false_negatives_2: 1095.0000 - true_negatives_2: 1658.0000 - sensitivity_at_specificity_2: 0.0712 - val_loss: 0.6932 - val_accuracy: 0.5531 - val_recall: 0.5531 - val_precision: 0.5531 - val_auc_2: 0.4165 - val_false_positives_2: 143.0000 - val_true_positives_2: 177.0000 - val_false_negatives_2: 143.0000 - val_true_negatives_2: 177.0000 - val_sensitivity_at_specificity_2: 0.0844\n",
      "Epoch 9/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6927 - accuracy: 0.6015 - recall: 0.6060 - precision: 0.6060 - auc_2: 0.4571 - false_positives_2: 1097.0000 - true_positives_2: 1656.0000 - false_negatives_2: 1097.0000 - true_negatives_2: 1656.0000 - sensitivity_at_specificity_2: 0.0995 - val_loss: 0.6933 - val_accuracy: 0.5375 - val_recall: 0.5375 - val_precision: 0.5375 - val_auc_2: 0.4147 - val_false_positives_2: 148.0000 - val_true_positives_2: 172.0000 - val_false_negatives_2: 148.0000 - val_true_negatives_2: 172.0000 - val_sensitivity_at_specificity_2: 0.1094\n",
      "Epoch 10/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6926 - accuracy: 0.6015 - recall: 0.5948 - precision: 0.5948 - auc_2: 0.4694 - false_positives_2: 1097.0000 - true_positives_2: 1656.0000 - false_negatives_2: 1097.0000 - true_negatives_2: 1656.0000 - sensitivity_at_specificity_2: 0.0865 - val_loss: 0.6931 - val_accuracy: 0.5469 - val_recall: 0.5469 - val_precision: 0.5469 - val_auc_2: 0.4235 - val_false_positives_2: 145.0000 - val_true_positives_2: 175.0000 - val_false_negatives_2: 145.0000 - val_true_negatives_2: 175.0000 - val_sensitivity_at_specificity_2: 0.0812\n",
      "Epoch 11/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6925 - accuracy: 0.5997 - recall: 0.5930 - precision: 0.5930 - auc_2: 0.4736 - false_positives_2: 1102.0000 - true_positives_2: 1651.0000 - false_negatives_2: 1102.0000 - true_negatives_2: 1651.0000 - sensitivity_at_specificity_2: 0.0923 - val_loss: 0.6930 - val_accuracy: 0.5469 - val_recall: 0.5469 - val_precision: 0.5469 - val_auc_2: 0.4082 - val_false_positives_2: 145.0000 - val_true_positives_2: 175.0000 - val_false_negatives_2: 145.0000 - val_true_negatives_2: 175.0000 - val_sensitivity_at_specificity_2: 0.0437\n",
      "Epoch 12/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6925 - accuracy: 0.6015 - recall: 0.6060 - precision: 0.6060 - auc_2: 0.4767 - false_positives_2: 1097.0000 - true_positives_2: 1656.0000 - false_negatives_2: 1097.0000 - true_negatives_2: 1656.0000 - sensitivity_at_specificity_2: 0.0854 - val_loss: 0.6929 - val_accuracy: 0.5625 - val_recall: 0.5625 - val_precision: 0.5625 - val_auc_2: 0.4317 - val_false_positives_2: 140.0000 - val_true_positives_2: 180.0000 - val_false_negatives_2: 140.0000 - val_true_negatives_2: 180.0000 - val_sensitivity_at_specificity_2: 0.0688\n",
      "Epoch 13/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6924 - accuracy: 0.6019 - recall: 0.5952 - precision: 0.5952 - auc_2: 0.4915 - false_positives_2: 1096.0000 - true_positives_2: 1657.0000 - false_negatives_2: 1096.0000 - true_negatives_2: 1657.0000 - sensitivity_at_specificity_2: 0.0821 - val_loss: 0.6928 - val_accuracy: 0.5500 - val_recall: 0.5500 - val_precision: 0.5500 - val_auc_2: 0.4350 - val_false_positives_2: 144.0000 - val_true_positives_2: 176.0000 - val_false_negatives_2: 144.0000 - val_true_negatives_2: 176.0000 - val_sensitivity_at_specificity_2: 0.0500\n",
      "Epoch 14/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.6923 - accuracy: 0.6019 - recall: 0.6063 - precision: 0.6065 - auc_2: 0.5016 - false_positives_2: 1095.0000 - true_positives_2: 1657.0000 - false_negatives_2: 1096.0000 - true_negatives_2: 1658.0000 - sensitivity_at_specificity_2: 0.0825 - val_loss: 0.6928 - val_accuracy: 0.5406 - val_recall: 0.5406 - val_precision: 0.5406 - val_auc_2: 0.4691 - val_false_positives_2: 147.0000 - val_true_positives_2: 173.0000 - val_false_negatives_2: 147.0000 - val_true_negatives_2: 173.0000 - val_sensitivity_at_specificity_2: 0.0750\n",
      "Epoch 15/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6922 - accuracy: 0.6023 - recall: 0.5955 - precision: 0.5960 - auc_2: 0.5178 - false_positives_2: 1093.0000 - true_positives_2: 1658.0000 - false_negatives_2: 1095.0000 - true_negatives_2: 1660.0000 - sensitivity_at_specificity_2: 0.0843 - val_loss: 0.6926 - val_accuracy: 0.5562 - val_recall: 0.5562 - val_precision: 0.5562 - val_auc_2: 0.4912 - val_false_positives_2: 142.0000 - val_true_positives_2: 178.0000 - val_false_negatives_2: 142.0000 - val_true_negatives_2: 178.0000 - val_sensitivity_at_specificity_2: 0.0906\n",
      "Epoch 16/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6922 - accuracy: 0.5993 - recall: 0.6038 - precision: 0.6056 - auc_2: 0.5202 - false_positives_2: 1095.0000 - true_positives_2: 1650.0000 - false_negatives_2: 1103.0000 - true_negatives_2: 1658.0000 - sensitivity_at_specificity_2: 0.0835 - val_loss: 0.6926 - val_accuracy: 0.5437 - val_recall: 0.5437 - val_precision: 0.5454 - val_auc_2: 0.4953 - val_false_positives_2: 145.0000 - val_true_positives_2: 174.0000 - val_false_negatives_2: 146.0000 - val_true_negatives_2: 175.0000 - val_sensitivity_at_specificity_2: 0.0938\n",
      "Epoch 17/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6921 - accuracy: 0.5997 - recall: 0.6042 - precision: 0.6060 - auc_2: 0.5450 - false_positives_2: 1094.0000 - true_positives_2: 1651.0000 - false_negatives_2: 1102.0000 - true_negatives_2: 1659.0000 - sensitivity_at_specificity_2: 0.1101 - val_loss: 0.6925 - val_accuracy: 0.5531 - val_recall: 0.5531 - val_precision: 0.5564 - val_auc_2: 0.5267 - val_false_positives_2: 141.0000 - val_true_positives_2: 177.0000 - val_false_negatives_2: 143.0000 - val_true_negatives_2: 179.0000 - val_sensitivity_at_specificity_2: 0.1156\n",
      "Epoch 18/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.6921 - accuracy: 0.5993 - recall: 0.5927 - precision: 0.5964 - auc_2: 0.5553 - false_positives_2: 1086.0000 - true_positives_2: 1650.0000 - false_negatives_2: 1103.0000 - true_negatives_2: 1667.0000 - sensitivity_at_specificity_2: 0.1210 - val_loss: 0.6925 - val_accuracy: 0.5469 - val_recall: 0.5469 - val_precision: 0.5504 - val_auc_2: 0.5400 - val_false_positives_2: 143.0000 - val_true_positives_2: 175.0000 - val_false_negatives_2: 145.0000 - val_true_negatives_2: 177.0000 - val_sensitivity_at_specificity_2: 0.1094\n",
      "Epoch 19/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6920 - accuracy: 0.6004 - recall: 0.6049 - precision: 0.6128 - auc_2: 0.5628 - false_positives_2: 1064.0000 - true_positives_2: 1653.0000 - false_negatives_2: 1100.0000 - true_negatives_2: 1689.0000 - sensitivity_at_specificity_2: 0.1057 - val_loss: 0.6924 - val_accuracy: 0.5469 - val_recall: 0.5469 - val_precision: 0.5606 - val_auc_2: 0.5558 - val_false_positives_2: 137.0000 - val_true_positives_2: 175.0000 - val_false_negatives_2: 145.0000 - val_true_negatives_2: 183.0000 - val_sensitivity_at_specificity_2: 0.1000\n",
      "Epoch 20/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6919 - accuracy: 0.6012 - recall: 0.5945 - precision: 0.6063 - auc_2: 0.5813 - false_positives_2: 1045.0000 - true_positives_2: 1655.0000 - false_negatives_2: 1098.0000 - true_negatives_2: 1708.0000 - sensitivity_at_specificity_2: 0.1213 - val_loss: 0.6921 - val_accuracy: 0.5688 - val_recall: 0.5688 - val_precision: 0.5890 - val_auc_2: 0.5773 - val_false_positives_2: 127.0000 - val_true_positives_2: 182.0000 - val_false_negatives_2: 138.0000 - val_true_negatives_2: 193.0000 - val_sensitivity_at_specificity_2: 0.1250\n",
      "Epoch 21/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6918 - accuracy: 0.6001 - recall: 0.6045 - precision: 0.6194 - auc_2: 0.5979 - false_positives_2: 1034.0000 - true_positives_2: 1652.0000 - false_negatives_2: 1101.0000 - true_negatives_2: 1719.0000 - sensitivity_at_specificity_2: 0.1286 - val_loss: 0.6922 - val_accuracy: 0.5500 - val_recall: 0.5469 - val_precision: 0.5701 - val_auc_2: 0.5764 - val_false_positives_2: 132.0000 - val_true_positives_2: 175.0000 - val_false_negatives_2: 145.0000 - val_true_negatives_2: 188.0000 - val_sensitivity_at_specificity_2: 0.1344\n",
      "Epoch 22/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.6917 - accuracy: 0.6023 - recall: 0.6063 - precision: 0.6263 - auc_2: 0.6137 - false_positives_2: 1010.0000 - true_positives_2: 1657.0000 - false_negatives_2: 1096.0000 - true_negatives_2: 1743.0000 - sensitivity_at_specificity_2: 0.1493 - val_loss: 0.6922 - val_accuracy: 0.5406 - val_recall: 0.5406 - val_precision: 0.5649 - val_auc_2: 0.5925 - val_false_positives_2: 133.0000 - val_true_positives_2: 173.0000 - val_false_negatives_2: 147.0000 - val_true_negatives_2: 187.0000 - val_sensitivity_at_specificity_2: 0.1594\n",
      "Epoch 23/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6917 - accuracy: 0.6008 - recall: 0.6049 - precision: 0.6289 - auc_2: 0.6359 - false_positives_2: 993.0000 - true_positives_2: 1653.0000 - false_negatives_2: 1100.0000 - true_negatives_2: 1760.0000 - sensitivity_at_specificity_2: 0.1642 - val_loss: 0.6920 - val_accuracy: 0.5656 - val_recall: 0.5656 - val_precision: 0.5920 - val_auc_2: 0.6120 - val_false_positives_2: 125.0000 - val_true_positives_2: 181.0000 - val_false_negatives_2: 139.0000 - val_true_negatives_2: 195.0000 - val_sensitivity_at_specificity_2: 0.1344\n",
      "Epoch 24/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6916 - accuracy: 0.6041 - recall: 0.5948 - precision: 0.6202 - auc_2: 0.6453 - false_positives_2: 983.0000 - true_positives_2: 1656.0000 - false_negatives_2: 1097.0000 - true_negatives_2: 1770.0000 - sensitivity_at_specificity_2: 0.1798 - val_loss: 0.6921 - val_accuracy: 0.5562 - val_recall: 0.5562 - val_precision: 0.5895 - val_auc_2: 0.5948 - val_false_positives_2: 124.0000 - val_true_positives_2: 178.0000 - val_false_negatives_2: 142.0000 - val_true_negatives_2: 196.0000 - val_sensitivity_at_specificity_2: 0.1406\n",
      "Epoch 25/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6915 - accuracy: 0.6052 - recall: 0.6060 - precision: 0.6346 - auc_2: 0.6714 - false_positives_2: 970.0000 - true_positives_2: 1656.0000 - false_negatives_2: 1097.0000 - true_negatives_2: 1783.0000 - sensitivity_at_specificity_2: 0.2132 - val_loss: 0.6919 - val_accuracy: 0.5500 - val_recall: 0.5469 - val_precision: 0.5835 - val_auc_2: 0.6506 - val_false_positives_2: 126.0000 - val_true_positives_2: 175.0000 - val_false_negatives_2: 145.0000 - val_true_negatives_2: 194.0000 - val_sensitivity_at_specificity_2: 0.1906\n",
      "Epoch 26/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.6914 - accuracy: 0.6106 - recall: 0.5941 - precision: 0.6327 - auc_2: 0.6818 - false_positives_2: 932.0000 - true_positives_2: 1654.0000 - false_negatives_2: 1099.0000 - true_negatives_2: 1821.0000 - sensitivity_at_specificity_2: 0.2158 - val_loss: 0.6917 - val_accuracy: 0.5719 - val_recall: 0.5531 - val_precision: 0.6061 - val_auc_2: 0.6620 - val_false_positives_2: 116.0000 - val_true_positives_2: 177.0000 - val_false_negatives_2: 143.0000 - val_true_negatives_2: 204.0000 - val_sensitivity_at_specificity_2: 0.1656\n",
      "Epoch 27/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6914 - accuracy: 0.6219 - recall: 0.5963 - precision: 0.6476 - auc_2: 0.6742 - false_positives_2: 877.0000 - true_positives_2: 1660.0000 - false_negatives_2: 1093.0000 - true_negatives_2: 1876.0000 - sensitivity_at_specificity_2: 0.1943 - val_loss: 0.6918 - val_accuracy: 0.5594 - val_recall: 0.5281 - val_precision: 0.6007 - val_auc_2: 0.6494 - val_false_positives_2: 113.0000 - val_true_positives_2: 169.0000 - val_false_negatives_2: 151.0000 - val_true_negatives_2: 207.0000 - val_sensitivity_at_specificity_2: 0.1531\n",
      "Epoch 28/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6913 - accuracy: 0.6240 - recall: 0.6081 - precision: 0.6631 - auc_2: 0.6843 - false_positives_2: 858.0000 - true_positives_2: 1662.0000 - false_negatives_2: 1091.0000 - true_negatives_2: 1895.0000 - sensitivity_at_specificity_2: 0.2129 - val_loss: 0.6917 - val_accuracy: 0.5844 - val_recall: 0.5500 - val_precision: 0.6122 - val_auc_2: 0.6684 - val_false_positives_2: 110.0000 - val_true_positives_2: 176.0000 - val_false_negatives_2: 144.0000 - val_true_negatives_2: 210.0000 - val_sensitivity_at_specificity_2: 0.1688\n",
      "Epoch 29/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6912 - accuracy: 0.6346 - recall: 0.5999 - precision: 0.6615 - auc_2: 0.6922 - false_positives_2: 829.0000 - true_positives_2: 1670.0000 - false_negatives_2: 1083.0000 - true_negatives_2: 1924.0000 - sensitivity_at_specificity_2: 0.2107 - val_loss: 0.6917 - val_accuracy: 0.5813 - val_recall: 0.5406 - val_precision: 0.6193 - val_auc_2: 0.6485 - val_false_positives_2: 106.0000 - val_true_positives_2: 173.0000 - val_false_negatives_2: 147.0000 - val_true_negatives_2: 214.0000 - val_sensitivity_at_specificity_2: 0.1875\n",
      "Epoch 30/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.6912 - accuracy: 0.6418 - recall: 0.6103 - precision: 0.6758 - auc_2: 0.7075 - false_positives_2: 815.0000 - true_positives_2: 1668.0000 - false_negatives_2: 1085.0000 - true_negatives_2: 1938.0000 - sensitivity_at_specificity_2: 0.2252 - val_loss: 0.6915 - val_accuracy: 0.6031 - val_recall: 0.5688 - val_precision: 0.6366 - val_auc_2: 0.6790 - val_false_positives_2: 105.0000 - val_true_positives_2: 182.0000 - val_false_negatives_2: 138.0000 - val_true_negatives_2: 215.0000 - val_sensitivity_at_specificity_2: 0.2031\n",
      "Epoch 31/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6911 - accuracy: 0.6509 - recall: 0.6074 - precision: 0.6729 - auc_2: 0.7188 - false_positives_2: 793.0000 - true_positives_2: 1691.0000 - false_negatives_2: 1062.0000 - true_negatives_2: 1960.0000 - sensitivity_at_specificity_2: 0.2379 - val_loss: 0.6913 - val_accuracy: 0.6375 - val_recall: 0.5875 - val_precision: 0.6613 - val_auc_2: 0.6831 - val_false_positives_2: 95.0000 - val_true_positives_2: 188.0000 - val_false_negatives_2: 132.0000 - val_true_negatives_2: 225.0000 - val_sensitivity_at_specificity_2: 0.2031\n",
      "Epoch 32/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6910 - accuracy: 0.6673 - recall: 0.6272 - precision: 0.6953 - auc_2: 0.7143 - false_positives_2: 761.0000 - true_positives_2: 1715.0000 - false_negatives_2: 1038.0000 - true_negatives_2: 1992.0000 - sensitivity_at_specificity_2: 0.2234 - val_loss: 0.6913 - val_accuracy: 0.6344 - val_recall: 0.5750 - val_precision: 0.6555 - val_auc_2: 0.6969 - val_false_positives_2: 96.0000 - val_true_positives_2: 184.0000 - val_false_negatives_2: 136.0000 - val_true_negatives_2: 224.0000 - val_sensitivity_at_specificity_2: 0.1969\n",
      "Epoch 33/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6909 - accuracy: 0.6691 - recall: 0.6286 - precision: 0.6954 - auc_2: 0.7353 - false_positives_2: 765.0000 - true_positives_2: 1719.0000 - false_negatives_2: 1034.0000 - true_negatives_2: 1988.0000 - sensitivity_at_specificity_2: 0.2641 - val_loss: 0.6913 - val_accuracy: 0.6187 - val_recall: 0.5719 - val_precision: 0.6382 - val_auc_2: 0.7214 - val_false_positives_2: 103.0000 - val_true_positives_2: 183.0000 - val_false_negatives_2: 137.0000 - val_true_negatives_2: 217.0000 - val_sensitivity_at_specificity_2: 0.2469\n",
      "Epoch 34/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.6909 - accuracy: 0.6633 - recall: 0.6268 - precision: 0.6915 - auc_2: 0.7560 - false_positives_2: 777.0000 - true_positives_2: 1714.0000 - false_negatives_2: 1039.0000 - true_negatives_2: 1976.0000 - sensitivity_at_specificity_2: 0.3004 - val_loss: 0.6912 - val_accuracy: 0.6344 - val_recall: 0.5875 - val_precision: 0.6619 - val_auc_2: 0.7011 - val_false_positives_2: 96.0000 - val_true_positives_2: 188.0000 - val_false_negatives_2: 132.0000 - val_true_negatives_2: 224.0000 - val_sensitivity_at_specificity_2: 0.2344\n",
      "Epoch 35/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6908 - accuracy: 0.6760 - recall: 0.6236 - precision: 0.6900 - auc_2: 0.7593 - false_positives_2: 748.0000 - true_positives_2: 1736.0000 - false_negatives_2: 1017.0000 - true_negatives_2: 2005.0000 - sensitivity_at_specificity_2: 0.2928 - val_loss: 0.6911 - val_accuracy: 0.6500 - val_recall: 0.5844 - val_precision: 0.6769 - val_auc_2: 0.7228 - val_false_positives_2: 90.0000 - val_true_positives_2: 187.0000 - val_false_negatives_2: 133.0000 - val_true_negatives_2: 230.0000 - val_sensitivity_at_specificity_2: 0.2344\n",
      "Wall time: 4min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "ephocs=35\n",
    "                    \n",
    "\n",
    "steps_per_epoch=train_generator.n//train_generator.batch_size\n",
    "steps_per_epoch\n",
    "validation_steps=validation_generator.n//validation_generator.batch_size\n",
    "validation_steps\n",
    "mod = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=ephocs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_steps\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 154ms/step - loss: 0.6906 - accuracy: 0.6868 - recall: 0.6445 - precision: 0.7256 - auc_2: 0.7909 - false_positives_2: 86.0000 - true_positives_2: 224.0000 - false_negatives_2: 124.0000 - true_negatives_2: 262.0000 - sensitivity_at_specificity_2: 0.3218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.690595805644989,\n",
       " 0.6867815852165222,\n",
       " 0.6444805264472961,\n",
       " 0.7256469130516052,\n",
       " 0.7908574342727661,\n",
       " 86.0,\n",
       " 224.0,\n",
       " 124.0,\n",
       " 262.0,\n",
       " 0.3218390941619873]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet50(input_shape=[100,100]+ [1], weights=None, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(resnet.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=resnet.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 100, 100, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 1)  0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 50, 50, 64)   3200        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 50, 50, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 50, 50, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 52, 52, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 25, 25, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 25, 25, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 25, 25, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 25, 25, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 25, 25, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 25, 25, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 25, 25, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 25, 25, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 25, 25, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 25, 25, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 25, 25, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 25, 25, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 25, 25, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 25, 25, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 25, 25, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 13, 13, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 13, 13, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 13, 13, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 13, 13, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 13, 13, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 13, 13, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 13, 13, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 13, 13, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 13, 13, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 13, 13, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 13, 13, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 13, 13, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 13, 13, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 13, 13, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 13, 13, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 13, 13, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 13, 13, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 13, 13, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 7, 7, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 7, 7, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 7, 7, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 7, 7, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 7, 7, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 7, 7, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 7, 7, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 7, 7, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 7, 7, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 7, 7, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 7, 7, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 7, 7, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 7, 7, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 7, 7, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 7, 7, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 7, 7, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 7, 7, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 7, 7, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 7, 7, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 7, 7, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 7, 7, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 7, 7, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 32768)        0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            65538       flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,646,978\n",
      "Trainable params: 65,538\n",
      "Non-trainable params: 23,581,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# view the structure of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "optimum= Adam(learning_rate=0.000005)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimum, metrics=['accuracy',recall,precision,tf.keras.metrics.AUC(curve=\"ROC\"),tf.keras.metrics.FalsePositives(),tf.keras.metrics.TruePositives(),tf.keras.metrics.FalseNegatives(),tf.keras.metrics.TrueNegatives(),tf.keras.metrics.SensitivityAtSpecificity(0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "87/87 [==============================] - 12s 105ms/step - loss: 0.6369 - accuracy: 0.5449 - recall: 0.5172 - precision: 0.5915 - auc_5: 0.6597 - false_positives_5: 995.0000 - true_positives_5: 1409.0000 - false_negatives_5: 1344.0000 - true_negatives_5: 1758.0000 - sensitivity_at_specificity_5: 0.5826 - val_loss: 0.5947 - val_accuracy: 0.6438 - val_recall: 0.5875 - val_precision: 0.6747 - val_auc_5: 0.7631 - val_false_positives_5: 91.0000 - val_true_positives_5: 188.0000 - val_false_negatives_5: 132.0000 - val_true_negatives_5: 229.0000 - val_sensitivity_at_specificity_5: 0.7719\n",
      "Epoch 2/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.5885 - accuracy: 0.7827 - recall: 0.7062 - precision: 0.7920 - auc_5: 0.8534 - false_positives_5: 514.0000 - true_positives_5: 1966.0000 - false_negatives_5: 818.0000 - true_negatives_5: 2270.0000 - sensitivity_at_specificity_5: 0.9192 - val_loss: 0.5604 - val_accuracy: 0.9000 - val_recall: 0.8719 - val_precision: 0.8904 - val_auc_5: 0.9494 - val_false_positives_5: 35.0000 - val_true_positives_5: 279.0000 - val_false_negatives_5: 41.0000 - val_true_negatives_5: 285.0000 - val_sensitivity_at_specificity_5: 0.9844\n",
      "Epoch 3/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.5482 - accuracy: 0.9110 - recall: 0.8843 - precision: 0.9022 - auc_5: 0.9551 - false_positives_5: 270.0000 - true_positives_5: 2431.0000 - false_negatives_5: 322.0000 - true_negatives_5: 2483.0000 - sensitivity_at_specificity_5: 0.9855 - val_loss: 0.5223 - val_accuracy: 0.9250 - val_recall: 0.9094 - val_precision: 0.9114 - val_auc_5: 0.9689 - val_false_positives_5: 29.0000 - val_true_positives_5: 291.0000 - val_false_negatives_5: 29.0000 - val_true_negatives_5: 291.0000 - val_sensitivity_at_specificity_5: 0.9937\n",
      "Epoch 4/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.5134 - accuracy: 0.9357 - recall: 0.9242 - precision: 0.9308 - auc_5: 0.9734 - false_positives_5: 193.0000 - true_positives_5: 2542.0000 - false_negatives_5: 211.0000 - true_negatives_5: 2560.0000 - sensitivity_at_specificity_5: 0.9931 - val_loss: 0.4890 - val_accuracy: 0.9312 - val_recall: 0.9312 - val_precision: 0.9289 - val_auc_5: 0.9748 - val_false_positives_5: 23.0000 - val_true_positives_5: 298.0000 - val_false_negatives_5: 22.0000 - val_true_negatives_5: 297.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 5/35\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 0.4841 - accuracy: 0.9372 - recall: 0.9332 - precision: 0.9352 - auc_5: 0.9759 - false_positives_5: 182.0000 - true_positives_5: 2567.0000 - false_negatives_5: 186.0000 - true_negatives_5: 2571.0000 - sensitivity_at_specificity_5: 0.9931 - val_loss: 0.4658 - val_accuracy: 0.9281 - val_recall: 0.9281 - val_precision: 0.9169 - val_auc_5: 0.9700 - val_false_positives_5: 27.0000 - val_true_positives_5: 297.0000 - val_false_negatives_5: 23.0000 - val_true_negatives_5: 293.0000 - val_sensitivity_at_specificity_5: 0.9906\n",
      "Epoch 6/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.4590 - accuracy: 0.9339 - recall: 0.9296 - precision: 0.9325 - auc_5: 0.9743 - false_positives_5: 188.0000 - true_positives_5: 2557.0000 - false_negatives_5: 196.0000 - true_negatives_5: 2565.0000 - sensitivity_at_specificity_5: 0.9920 - val_loss: 0.4404 - val_accuracy: 0.9312 - val_recall: 0.9219 - val_precision: 0.9281 - val_auc_5: 0.9786 - val_false_positives_5: 23.0000 - val_true_positives_5: 295.0000 - val_false_negatives_5: 25.0000 - val_true_negatives_5: 297.0000 - val_sensitivity_at_specificity_5: 1.0000\n",
      "Epoch 7/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.4354 - accuracy: 0.9382 - recall: 0.9325 - precision: 0.9397 - auc_5: 0.9769 - false_positives_5: 167.0000 - true_positives_5: 2565.0000 - false_negatives_5: 188.0000 - true_negatives_5: 2586.0000 - sensitivity_at_specificity_5: 0.9924 - val_loss: 0.4152 - val_accuracy: 0.9281 - val_recall: 0.9187 - val_precision: 0.9217 - val_auc_5: 0.9764 - val_false_positives_5: 25.0000 - val_true_positives_5: 294.0000 - val_false_negatives_5: 26.0000 - val_true_negatives_5: 295.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 8/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.4179 - accuracy: 0.9350 - recall: 0.9343 - precision: 0.9370 - auc_5: 0.9757 - false_positives_5: 176.0000 - true_positives_5: 2570.0000 - false_negatives_5: 183.0000 - true_negatives_5: 2577.0000 - sensitivity_at_specificity_5: 0.9909 - val_loss: 0.4030 - val_accuracy: 0.9312 - val_recall: 0.9312 - val_precision: 0.9351 - val_auc_5: 0.9740 - val_false_positives_5: 21.0000 - val_true_positives_5: 298.0000 - val_false_negatives_5: 22.0000 - val_true_negatives_5: 299.0000 - val_sensitivity_at_specificity_5: 0.9937\n",
      "Epoch 9/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.4000 - accuracy: 0.9343 - recall: 0.9346 - precision: 0.9366 - auc_5: 0.9775 - false_positives_5: 177.0000 - true_positives_5: 2571.0000 - false_negatives_5: 182.0000 - true_negatives_5: 2576.0000 - sensitivity_at_specificity_5: 0.9924 - val_loss: 0.3894 - val_accuracy: 0.9250 - val_recall: 0.9250 - val_precision: 0.9284 - val_auc_5: 0.9765 - val_false_positives_5: 23.0000 - val_true_positives_5: 296.0000 - val_false_negatives_5: 24.0000 - val_true_negatives_5: 297.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 10/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.3829 - accuracy: 0.9335 - recall: 0.9339 - precision: 0.9347 - auc_5: 0.9770 - false_positives_5: 182.0000 - true_positives_5: 2569.0000 - false_negatives_5: 184.0000 - true_negatives_5: 2571.0000 - sensitivity_at_specificity_5: 0.9924 - val_loss: 0.3740 - val_accuracy: 0.9187 - val_recall: 0.9187 - val_precision: 0.9158 - val_auc_5: 0.9763 - val_false_positives_5: 27.0000 - val_true_positives_5: 294.0000 - val_false_negatives_5: 26.0000 - val_true_negatives_5: 293.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 11/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.3690 - accuracy: 0.9401 - recall: 0.9422 - precision: 0.9409 - auc_5: 0.9787 - false_positives_5: 165.0000 - true_positives_5: 2592.0000 - false_negatives_5: 161.0000 - true_negatives_5: 2588.0000 - sensitivity_at_specificity_5: 0.9931 - val_loss: 0.3639 - val_accuracy: 0.9344 - val_recall: 0.9312 - val_precision: 0.9314 - val_auc_5: 0.9764 - val_false_positives_5: 22.0000 - val_true_positives_5: 298.0000 - val_false_negatives_5: 22.0000 - val_true_negatives_5: 298.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 12/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.3573 - accuracy: 0.9361 - recall: 0.9357 - precision: 0.9365 - auc_5: 0.9775 - false_positives_5: 177.0000 - true_positives_5: 2574.0000 - false_negatives_5: 179.0000 - true_negatives_5: 2576.0000 - sensitivity_at_specificity_5: 0.9920 - val_loss: 0.3407 - val_accuracy: 0.9344 - val_recall: 0.9312 - val_precision: 0.9401 - val_auc_5: 0.9837 - val_false_positives_5: 19.0000 - val_true_positives_5: 298.0000 - val_false_negatives_5: 22.0000 - val_true_negatives_5: 301.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 13/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.3446 - accuracy: 0.9401 - recall: 0.9400 - precision: 0.9396 - auc_5: 0.9789 - false_positives_5: 169.0000 - true_positives_5: 2586.0000 - false_negatives_5: 167.0000 - true_negatives_5: 2584.0000 - sensitivity_at_specificity_5: 0.9924 - val_loss: 0.3364 - val_accuracy: 0.9312 - val_recall: 0.9219 - val_precision: 0.9311 - val_auc_5: 0.9797 - val_false_positives_5: 22.0000 - val_true_positives_5: 295.0000 - val_false_negatives_5: 25.0000 - val_true_negatives_5: 298.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 14/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.3339 - accuracy: 0.9390 - recall: 0.9375 - precision: 0.9410 - auc_5: 0.9799 - false_positives_5: 164.0000 - true_positives_5: 2579.0000 - false_negatives_5: 174.0000 - true_negatives_5: 2589.0000 - sensitivity_at_specificity_5: 0.9935 - val_loss: 0.3311 - val_accuracy: 0.9219 - val_recall: 0.9219 - val_precision: 0.9221 - val_auc_5: 0.9759 - val_false_positives_5: 25.0000 - val_true_positives_5: 295.0000 - val_false_negatives_5: 25.0000 - val_true_negatives_5: 295.0000 - val_sensitivity_at_specificity_5: 0.9937\n",
      "Epoch 15/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.3243 - accuracy: 0.9379 - recall: 0.9382 - precision: 0.9370 - auc_5: 0.9791 - false_positives_5: 176.0000 - true_positives_5: 2581.0000 - false_negatives_5: 172.0000 - true_negatives_5: 2577.0000 - sensitivity_at_specificity_5: 0.9924 - val_loss: 0.3232 - val_accuracy: 0.9250 - val_recall: 0.9281 - val_precision: 0.9252 - val_auc_5: 0.9770 - val_false_positives_5: 24.0000 - val_true_positives_5: 297.0000 - val_false_negatives_5: 23.0000 - val_true_negatives_5: 296.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 16/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.3148 - accuracy: 0.9390 - recall: 0.9397 - precision: 0.9366 - auc_5: 0.9794 - false_positives_5: 178.0000 - true_positives_5: 2585.0000 - false_negatives_5: 168.0000 - true_negatives_5: 2575.0000 - sensitivity_at_specificity_5: 0.9927 - val_loss: 0.3118 - val_accuracy: 0.9281 - val_recall: 0.9312 - val_precision: 0.9312 - val_auc_5: 0.9800 - val_false_positives_5: 22.0000 - val_true_positives_5: 298.0000 - val_false_negatives_5: 22.0000 - val_true_negatives_5: 298.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 17/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.3074 - accuracy: 0.9379 - recall: 0.9411 - precision: 0.9382 - auc_5: 0.9790 - false_positives_5: 173.0000 - true_positives_5: 2589.0000 - false_negatives_5: 164.0000 - true_negatives_5: 2580.0000 - sensitivity_at_specificity_5: 0.9942 - val_loss: 0.3061 - val_accuracy: 0.9312 - val_recall: 0.9344 - val_precision: 0.9374 - val_auc_5: 0.9809 - val_false_positives_5: 20.0000 - val_true_positives_5: 299.0000 - val_false_negatives_5: 21.0000 - val_true_negatives_5: 300.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 18/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.2993 - accuracy: 0.9430 - recall: 0.9447 - precision: 0.9438 - auc_5: 0.9809 - false_positives_5: 157.0000 - true_positives_5: 2599.0000 - false_negatives_5: 154.0000 - true_negatives_5: 2596.0000 - sensitivity_at_specificity_5: 0.9935 - val_loss: 0.2899 - val_accuracy: 0.9406 - val_recall: 0.9469 - val_precision: 0.9354 - val_auc_5: 0.9821 - val_false_positives_5: 21.0000 - val_true_positives_5: 303.0000 - val_false_negatives_5: 17.0000 - val_true_negatives_5: 299.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 19/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.2909 - accuracy: 0.9437 - recall: 0.9440 - precision: 0.9415 - auc_5: 0.9804 - false_positives_5: 164.0000 - true_positives_5: 2597.0000 - false_negatives_5: 156.0000 - true_negatives_5: 2589.0000 - sensitivity_at_specificity_5: 0.9935 - val_loss: 0.2796 - val_accuracy: 0.9406 - val_recall: 0.9344 - val_precision: 0.9403 - val_auc_5: 0.9845 - val_false_positives_5: 19.0000 - val_true_positives_5: 299.0000 - val_false_negatives_5: 21.0000 - val_true_negatives_5: 301.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 20/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.2843 - accuracy: 0.9437 - recall: 0.9443 - precision: 0.9439 - auc_5: 0.9815 - false_positives_5: 157.0000 - true_positives_5: 2598.0000 - false_negatives_5: 155.0000 - true_negatives_5: 2596.0000 - sensitivity_at_specificity_5: 0.9938 - val_loss: 0.2859 - val_accuracy: 0.9281 - val_recall: 0.9281 - val_precision: 0.9283 - val_auc_5: 0.9788 - val_false_positives_5: 23.0000 - val_true_positives_5: 297.0000 - val_false_negatives_5: 23.0000 - val_true_negatives_5: 297.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 21/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.2775 - accuracy: 0.9441 - recall: 0.9450 - precision: 0.9431 - auc_5: 0.9812 - false_positives_5: 159.0000 - true_positives_5: 2600.0000 - false_negatives_5: 153.0000 - true_negatives_5: 2594.0000 - sensitivity_at_specificity_5: 0.9938 - val_loss: 0.2724 - val_accuracy: 0.9344 - val_recall: 0.9375 - val_precision: 0.9261 - val_auc_5: 0.9833 - val_false_positives_5: 24.0000 - val_true_positives_5: 300.0000 - val_false_negatives_5: 20.0000 - val_true_negatives_5: 296.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 22/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.2709 - accuracy: 0.9473 - recall: 0.9479 - precision: 0.9460 - auc_5: 0.9818 - false_positives_5: 151.0000 - true_positives_5: 2608.0000 - false_negatives_5: 145.0000 - true_negatives_5: 2602.0000 - sensitivity_at_specificity_5: 0.9946 - val_loss: 0.2725 - val_accuracy: 0.9375 - val_recall: 0.9344 - val_precision: 0.9404 - val_auc_5: 0.9824 - val_false_positives_5: 19.0000 - val_true_positives_5: 299.0000 - val_false_negatives_5: 21.0000 - val_true_negatives_5: 301.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 23/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.2657 - accuracy: 0.9462 - recall: 0.9450 - precision: 0.9464 - auc_5: 0.9829 - false_positives_5: 149.0000 - true_positives_5: 2600.0000 - false_negatives_5: 153.0000 - true_negatives_5: 2604.0000 - sensitivity_at_specificity_5: 0.9946 - val_loss: 0.2630 - val_accuracy: 0.9281 - val_recall: 0.9312 - val_precision: 0.9318 - val_auc_5: 0.9820 - val_false_positives_5: 22.0000 - val_true_positives_5: 298.0000 - val_false_negatives_5: 22.0000 - val_true_negatives_5: 298.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 24/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.2582 - accuracy: 0.9459 - recall: 0.9461 - precision: 0.9446 - auc_5: 0.9829 - false_positives_5: 155.0000 - true_positives_5: 2603.0000 - false_negatives_5: 150.0000 - true_negatives_5: 2598.0000 - sensitivity_at_specificity_5: 0.9946 - val_loss: 0.2584 - val_accuracy: 0.9281 - val_recall: 0.9312 - val_precision: 0.9313 - val_auc_5: 0.9816 - val_false_positives_5: 22.0000 - val_true_positives_5: 298.0000 - val_false_negatives_5: 22.0000 - val_true_negatives_5: 298.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 25/35\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.2548 - accuracy: 0.9462 - recall: 0.9447 - precision: 0.9451 - auc_5: 0.9823 - false_positives_5: 153.0000 - true_positives_5: 2599.0000 - false_negatives_5: 154.0000 - true_negatives_5: 2600.0000 - sensitivity_at_specificity_5: 0.9942 - val_loss: 0.2528 - val_accuracy: 0.9375 - val_recall: 0.9375 - val_precision: 0.9317 - val_auc_5: 0.9836 - val_false_positives_5: 22.0000 - val_true_positives_5: 300.0000 - val_false_negatives_5: 20.0000 - val_true_negatives_5: 298.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 26/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.2488 - accuracy: 0.9470 - recall: 0.9483 - precision: 0.9473 - auc_5: 0.9834 - false_positives_5: 147.0000 - true_positives_5: 2609.0000 - false_negatives_5: 144.0000 - true_negatives_5: 2606.0000 - sensitivity_at_specificity_5: 0.9942 - val_loss: 0.2507 - val_accuracy: 0.9250 - val_recall: 0.9281 - val_precision: 0.9253 - val_auc_5: 0.9819 - val_false_positives_5: 24.0000 - val_true_positives_5: 297.0000 - val_false_negatives_5: 23.0000 - val_true_negatives_5: 296.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 27/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.2468 - accuracy: 0.9466 - recall: 0.9461 - precision: 0.9472 - auc_5: 0.9826 - false_positives_5: 147.0000 - true_positives_5: 2603.0000 - false_negatives_5: 150.0000 - true_negatives_5: 2606.0000 - sensitivity_at_specificity_5: 0.9946 - val_loss: 0.2467 - val_accuracy: 0.9312 - val_recall: 0.9281 - val_precision: 0.9310 - val_auc_5: 0.9849 - val_false_positives_5: 22.0000 - val_true_positives_5: 297.0000 - val_false_negatives_5: 23.0000 - val_true_negatives_5: 298.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 28/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.2426 - accuracy: 0.9481 - recall: 0.9468 - precision: 0.9494 - auc_5: 0.9832 - false_positives_5: 141.0000 - true_positives_5: 2605.0000 - false_negatives_5: 148.0000 - true_negatives_5: 2612.0000 - sensitivity_at_specificity_5: 0.9953 - val_loss: 0.2458 - val_accuracy: 0.9281 - val_recall: 0.9312 - val_precision: 0.9289 - val_auc_5: 0.9825 - val_false_positives_5: 23.0000 - val_true_positives_5: 298.0000 - val_false_negatives_5: 22.0000 - val_true_negatives_5: 297.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 29/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.2379 - accuracy: 0.9470 - recall: 0.9476 - precision: 0.9482 - auc_5: 0.9831 - false_positives_5: 144.0000 - true_positives_5: 2607.0000 - false_negatives_5: 146.0000 - true_negatives_5: 2609.0000 - sensitivity_at_specificity_5: 0.9949 - val_loss: 0.2360 - val_accuracy: 0.9375 - val_recall: 0.9406 - val_precision: 0.9378 - val_auc_5: 0.9866 - val_false_positives_5: 20.0000 - val_true_positives_5: 301.0000 - val_false_negatives_5: 19.0000 - val_true_negatives_5: 300.0000 - val_sensitivity_at_specificity_5: 1.0000\n",
      "Epoch 30/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.2343 - accuracy: 0.9488 - recall: 0.9511 - precision: 0.9502 - auc_5: 0.9830 - false_positives_5: 139.0000 - true_positives_5: 2617.0000 - false_negatives_5: 136.0000 - true_negatives_5: 2614.0000 - sensitivity_at_specificity_5: 0.9946 - val_loss: 0.2408 - val_accuracy: 0.9344 - val_recall: 0.9438 - val_precision: 0.9325 - val_auc_5: 0.9827 - val_false_positives_5: 22.0000 - val_true_positives_5: 302.0000 - val_false_negatives_5: 18.0000 - val_true_negatives_5: 298.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 31/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.2311 - accuracy: 0.9477 - recall: 0.9501 - precision: 0.9489 - auc_5: 0.9832 - false_positives_5: 143.0000 - true_positives_5: 2614.0000 - false_negatives_5: 139.0000 - true_negatives_5: 2610.0000 - sensitivity_at_specificity_5: 0.9946 - val_loss: 0.2277 - val_accuracy: 0.9406 - val_recall: 0.9469 - val_precision: 0.9357 - val_auc_5: 0.9866 - val_false_positives_5: 21.0000 - val_true_positives_5: 303.0000 - val_false_negatives_5: 17.0000 - val_true_negatives_5: 299.0000 - val_sensitivity_at_specificity_5: 1.0000\n",
      "Epoch 32/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.2267 - accuracy: 0.9481 - recall: 0.9494 - precision: 0.9477 - auc_5: 0.9834 - false_positives_5: 146.0000 - true_positives_5: 2612.0000 - false_negatives_5: 141.0000 - true_negatives_5: 2607.0000 - sensitivity_at_specificity_5: 0.9956 - val_loss: 0.2225 - val_accuracy: 0.9375 - val_recall: 0.9438 - val_precision: 0.9380 - val_auc_5: 0.9860 - val_false_positives_5: 20.0000 - val_true_positives_5: 302.0000 - val_false_negatives_5: 18.0000 - val_true_negatives_5: 300.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 33/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.2218 - accuracy: 0.9539 - recall: 0.9544 - precision: 0.9531 - auc_5: 0.9855 - false_positives_5: 131.0000 - true_positives_5: 2626.0000 - false_negatives_5: 127.0000 - true_negatives_5: 2622.0000 - sensitivity_at_specificity_5: 0.9956 - val_loss: 0.2217 - val_accuracy: 0.9375 - val_recall: 0.9406 - val_precision: 0.9378 - val_auc_5: 0.9858 - val_false_positives_5: 20.0000 - val_true_positives_5: 301.0000 - val_false_negatives_5: 19.0000 - val_true_negatives_5: 300.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 34/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.2197 - accuracy: 0.9481 - recall: 0.9497 - precision: 0.9494 - auc_5: 0.9844 - false_positives_5: 141.0000 - true_positives_5: 2613.0000 - false_negatives_5: 140.0000 - true_negatives_5: 2612.0000 - sensitivity_at_specificity_5: 0.9960 - val_loss: 0.2220 - val_accuracy: 0.9438 - val_recall: 0.9438 - val_precision: 0.9381 - val_auc_5: 0.9843 - val_false_positives_5: 20.0000 - val_true_positives_5: 302.0000 - val_false_negatives_5: 18.0000 - val_true_negatives_5: 300.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Epoch 35/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.2168 - accuracy: 0.9499 - recall: 0.9508 - precision: 0.9503 - auc_5: 0.9841 - false_positives_5: 139.0000 - true_positives_5: 2616.0000 - false_negatives_5: 137.0000 - true_negatives_5: 2614.0000 - sensitivity_at_specificity_5: 0.9949 - val_loss: 0.2268 - val_accuracy: 0.9344 - val_recall: 0.9375 - val_precision: 0.9348 - val_auc_5: 0.9821 - val_false_positives_5: 21.0000 - val_true_positives_5: 300.0000 - val_false_negatives_5: 20.0000 - val_true_negatives_5: 299.0000 - val_sensitivity_at_specificity_5: 0.9969\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "ephocs=35\n",
    "                    \n",
    "\n",
    "steps_per_epoch=train_generator.n//train_generator.batch_size\n",
    "steps_per_epoch\n",
    "validation_steps=validation_generator.n//validation_generator.batch_size\n",
    "validation_steps\n",
    "mod = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=ephocs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_steps\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 113ms/step - loss: 0.2207 - accuracy: 0.9425 - recall: 0.9412 - precision: 0.9412 - auc_5: 0.9847 - false_positives_5: 20.0000 - true_positives_5: 328.0000 - false_negatives_5: 20.0000 - true_negatives_5: 328.0000 - sensitivity_at_specificity_5: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22066135704517365,\n",
       " 0.9425287246704102,\n",
       " 0.9411525726318359,\n",
       " 0.9411525726318359,\n",
       " 0.9847114682197571,\n",
       " 20.0,\n",
       " 328.0,\n",
       " 20.0,\n",
       " 328.0,\n",
       " 0.9971264600753784]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG16(input_shape=[100,100]+ [1], weights=None, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(vgg16.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=vgg16.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 100, 100, 1)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      640       \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 9218      \n",
      "=================================================================\n",
      "Total params: 14,722,754\n",
      "Trainable params: 9,218\n",
      "Non-trainable params: 14,713,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# view the structure of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "optimum= Adam(learning_rate=0.000005)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimum, metrics=['accuracy',recall,precision,tf.keras.metrics.AUC(curve=\"ROC\"),tf.keras.metrics.FalsePositives(),tf.keras.metrics.TruePositives(),tf.keras.metrics.FalseNegatives(),tf.keras.metrics.TrueNegatives(),tf.keras.metrics.SensitivityAtSpecificity(0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "87/87 [==============================] - 10s 97ms/step - loss: 0.6938 - accuracy: 0.6001 - recall: 0.1667 - precision: 0.4717 - auc_6: 0.5239 - false_positives_6: 521.0000 - true_positives_6: 464.0000 - false_negatives_6: 2289.0000 - true_negatives_6: 2232.0000 - sensitivity_at_specificity_6: 0.5376 - val_loss: 0.6944 - val_accuracy: 0.5437 - val_recall: 0.1437 - val_precision: 0.4717 - val_auc_6: 0.4882 - val_false_positives_6: 59.0000 - val_true_positives_6: 46.0000 - val_false_negatives_6: 274.0000 - val_true_negatives_6: 261.0000 - val_sensitivity_at_specificity_6: 0.4906\n",
      "Epoch 2/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6928 - accuracy: 0.6044 - recall: 0.1685 - precision: 0.5663 - auc_6: 0.5646 - false_positives_6: 354.0000 - true_positives_6: 469.0000 - false_negatives_6: 2284.0000 - true_negatives_6: 2399.0000 - sensitivity_at_specificity_6: 0.5369 - val_loss: 0.6930 - val_accuracy: 0.5750 - val_recall: 0.1250 - val_precision: 0.5272 - val_auc_6: 0.5520 - val_false_positives_6: 35.0000 - val_true_positives_6: 40.0000 - val_false_negatives_6: 280.0000 - val_true_negatives_6: 285.0000 - val_sensitivity_at_specificity_6: 0.5000\n",
      "Epoch 3/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6921 - accuracy: 0.6328 - recall: 0.0715 - precision: 0.5966 - auc_6: 0.6105 - false_positives_6: 118.0000 - true_positives_6: 199.0000 - false_negatives_6: 2554.0000 - true_negatives_6: 2635.0000 - sensitivity_at_specificity_6: 0.4693 - val_loss: 0.6922 - val_accuracy: 0.6250 - val_recall: 0.0562 - val_precision: 0.6433 - val_auc_6: 0.6016 - val_false_positives_6: 11.0000 - val_true_positives_6: 18.0000 - val_false_negatives_6: 302.0000 - val_true_negatives_6: 309.0000 - val_sensitivity_at_specificity_6: 0.4219\n",
      "Epoch 4/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6912 - accuracy: 0.7050 - recall: 0.0632 - precision: 0.7073 - auc_6: 0.6710 - false_positives_6: 57.0000 - true_positives_6: 176.0000 - false_negatives_6: 2577.0000 - true_negatives_6: 2696.0000 - sensitivity_at_specificity_6: 0.4755 - val_loss: 0.6909 - val_accuracy: 0.7563 - val_recall: 0.0750 - val_precision: 0.8583 - val_auc_6: 0.6879 - val_false_positives_6: 5.0000 - val_true_positives_6: 24.0000 - val_false_negatives_6: 296.0000 - val_true_negatives_6: 315.0000 - val_sensitivity_at_specificity_6: 0.4531\n",
      "Epoch 5/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6905 - accuracy: 0.7806 - recall: 0.0686 - precision: 0.8093 - auc_6: 0.7265 - false_positives_6: 30.0000 - true_positives_6: 191.0000 - false_negatives_6: 2562.0000 - true_negatives_6: 2723.0000 - sensitivity_at_specificity_6: 0.5223 - val_loss: 0.6902 - val_accuracy: 0.8031 - val_recall: 0.0875 - val_precision: 0.8667 - val_auc_6: 0.7535 - val_false_positives_6: 4.0000 - val_true_positives_6: 28.0000 - val_false_negatives_6: 292.0000 - val_true_negatives_6: 316.0000 - val_sensitivity_at_specificity_6: 0.5469\n",
      "Epoch 6/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6898 - accuracy: 0.8235 - recall: 0.1437 - precision: 0.8968 - auc_6: 0.7697 - false_positives_6: 38.0000 - true_positives_6: 400.0000 - false_negatives_6: 2353.0000 - true_negatives_6: 2715.0000 - sensitivity_at_specificity_6: 0.6066 - val_loss: 0.6894 - val_accuracy: 0.8000 - val_recall: 0.1625 - val_precision: 0.9217 - val_auc_6: 0.7648 - val_false_positives_6: 4.0000 - val_true_positives_6: 52.0000 - val_false_negatives_6: 268.0000 - val_true_negatives_6: 316.0000 - val_sensitivity_at_specificity_6: 0.5969\n",
      "Epoch 7/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6890 - accuracy: 0.8409 - recall: 0.1911 - precision: 0.9420 - auc_6: 0.7986 - false_positives_6: 24.0000 - true_positives_6: 532.0000 - false_negatives_6: 2221.0000 - true_negatives_6: 2729.0000 - sensitivity_at_specificity_6: 0.6455 - val_loss: 0.6889 - val_accuracy: 0.8062 - val_recall: 0.1937 - val_precision: 0.9044 - val_auc_6: 0.7897 - val_false_positives_6: 7.0000 - val_true_positives_6: 62.0000 - val_false_negatives_6: 258.0000 - val_true_negatives_6: 313.0000 - val_sensitivity_at_specificity_6: 0.6406\n",
      "Epoch 8/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6883 - accuracy: 0.8307 - recall: 0.2518 - precision: 0.9227 - auc_6: 0.8058 - false_positives_6: 47.0000 - true_positives_6: 701.0000 - false_negatives_6: 2052.0000 - true_negatives_6: 2706.0000 - sensitivity_at_specificity_6: 0.6760 - val_loss: 0.6880 - val_accuracy: 0.7812 - val_recall: 0.2906 - val_precision: 0.9074 - val_auc_6: 0.7825 - val_false_positives_6: 9.0000 - val_true_positives_6: 93.0000 - val_false_negatives_6: 227.0000 - val_true_negatives_6: 311.0000 - val_sensitivity_at_specificity_6: 0.6469\n",
      "Epoch 9/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6875 - accuracy: 0.7821 - recall: 0.3161 - precision: 0.8866 - auc_6: 0.7999 - false_positives_6: 105.0000 - true_positives_6: 849.0000 - false_negatives_6: 1904.0000 - true_negatives_6: 2648.0000 - sensitivity_at_specificity_6: 0.6843 - val_loss: 0.6871 - val_accuracy: 0.8000 - val_recall: 0.3531 - val_precision: 0.8860 - val_auc_6: 0.8043 - val_false_positives_6: 13.0000 - val_true_positives_6: 113.0000 - val_false_negatives_6: 207.0000 - val_true_negatives_6: 307.0000 - val_sensitivity_at_specificity_6: 0.7031\n",
      "Epoch 10/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6870 - accuracy: 0.8173 - recall: 0.3599 - precision: 0.9015 - auc_6: 0.8189 - false_positives_6: 97.0000 - true_positives_6: 1002.0000 - false_negatives_6: 1751.0000 - true_negatives_6: 2656.0000 - sensitivity_at_specificity_6: 0.7323 - val_loss: 0.6863 - val_accuracy: 0.8219 - val_recall: 0.4156 - val_precision: 0.9188 - val_auc_6: 0.8374 - val_false_positives_6: 11.0000 - val_true_positives_6: 133.0000 - val_false_negatives_6: 187.0000 - val_true_negatives_6: 309.0000 - val_sensitivity_at_specificity_6: 0.7594\n",
      "Epoch 11/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6861 - accuracy: 0.8216 - recall: 0.4217 - precision: 0.9160 - auc_6: 0.8391 - false_positives_6: 109.0000 - true_positives_6: 1143.0000 - false_negatives_6: 1610.0000 - true_negatives_6: 2644.0000 - sensitivity_at_specificity_6: 0.7799 - val_loss: 0.6850 - val_accuracy: 0.7594 - val_recall: 0.4313 - val_precision: 0.8429 - val_auc_6: 0.8289 - val_false_positives_6: 25.0000 - val_true_positives_6: 138.0000 - val_false_negatives_6: 182.0000 - val_true_negatives_6: 295.0000 - val_sensitivity_at_specificity_6: 0.7312\n",
      "Epoch 12/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6853 - accuracy: 0.7900 - recall: 0.4386 - precision: 0.8580 - auc_6: 0.8313 - false_positives_6: 182.0000 - true_positives_6: 1221.0000 - false_negatives_6: 1532.0000 - true_negatives_6: 2571.0000 - sensitivity_at_specificity_6: 0.7777 - val_loss: 0.6848 - val_accuracy: 0.7844 - val_recall: 0.4625 - val_precision: 0.8551 - val_auc_6: 0.8247 - val_false_positives_6: 26.0000 - val_true_positives_6: 148.0000 - val_false_negatives_6: 172.0000 - val_true_negatives_6: 294.0000 - val_sensitivity_at_specificity_6: 0.7625\n",
      "Epoch 13/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6847 - accuracy: 0.8064 - recall: 0.4817 - precision: 0.8631 - auc_6: 0.8419 - false_positives_6: 194.0000 - true_positives_6: 1341.0000 - false_negatives_6: 1412.0000 - true_negatives_6: 2559.0000 - sensitivity_at_specificity_6: 0.8049 - val_loss: 0.6833 - val_accuracy: 0.8281 - val_recall: 0.5406 - val_precision: 0.8641 - val_auc_6: 0.8589 - val_false_positives_6: 26.0000 - val_true_positives_6: 173.0000 - val_false_negatives_6: 147.0000 - val_true_negatives_6: 294.0000 - val_sensitivity_at_specificity_6: 0.7969\n",
      "Epoch 14/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6841 - accuracy: 0.8017 - recall: 0.5269 - precision: 0.8607 - auc_6: 0.8438 - false_positives_6: 234.0000 - true_positives_6: 1436.0000 - false_negatives_6: 1317.0000 - true_negatives_6: 2519.0000 - sensitivity_at_specificity_6: 0.8144 - val_loss: 0.6825 - val_accuracy: 0.7719 - val_recall: 0.5531 - val_precision: 0.8233 - val_auc_6: 0.8442 - val_false_positives_6: 38.0000 - val_true_positives_6: 177.0000 - val_false_negatives_6: 143.0000 - val_true_negatives_6: 282.0000 - val_sensitivity_at_specificity_6: 0.8062\n",
      "Epoch 15/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6833 - accuracy: 0.7479 - recall: 0.5133 - precision: 0.8124 - auc_6: 0.8186 - false_positives_6: 326.0000 - true_positives_6: 1398.0000 - false_negatives_6: 1355.0000 - true_negatives_6: 2427.0000 - sensitivity_at_specificity_6: 0.7821 - val_loss: 0.6822 - val_accuracy: 0.7406 - val_recall: 0.5500 - val_precision: 0.7944 - val_auc_6: 0.8101 - val_false_positives_6: 45.0000 - val_true_positives_6: 176.0000 - val_false_negatives_6: 144.0000 - val_true_negatives_6: 275.0000 - val_sensitivity_at_specificity_6: 0.7563\n",
      "Epoch 16/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6826 - accuracy: 0.7159 - recall: 0.5083 - precision: 0.7667 - auc_6: 0.7971 - false_positives_6: 430.0000 - true_positives_6: 1384.0000 - false_negatives_6: 1369.0000 - true_negatives_6: 2323.0000 - sensitivity_at_specificity_6: 0.7614 - val_loss: 0.6814 - val_accuracy: 0.7219 - val_recall: 0.5281 - val_precision: 0.7548 - val_auc_6: 0.8032 - val_false_positives_6: 55.0000 - val_true_positives_6: 169.0000 - val_false_negatives_6: 151.0000 - val_true_negatives_6: 265.0000 - val_sensitivity_at_specificity_6: 0.7406\n",
      "Epoch 17/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6819 - accuracy: 0.7258 - recall: 0.5370 - precision: 0.7713 - auc_6: 0.8040 - false_positives_6: 440.0000 - true_positives_6: 1464.0000 - false_negatives_6: 1289.0000 - true_negatives_6: 2313.0000 - sensitivity_at_specificity_6: 0.7788 - val_loss: 0.6801 - val_accuracy: 0.6969 - val_recall: 0.5531 - val_precision: 0.7398 - val_auc_6: 0.7952 - val_false_positives_6: 62.0000 - val_true_positives_6: 177.0000 - val_false_negatives_6: 143.0000 - val_true_negatives_6: 258.0000 - val_sensitivity_at_specificity_6: 0.7375\n",
      "Epoch 18/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6813 - accuracy: 0.6800 - recall: 0.5194 - precision: 0.7186 - auc_6: 0.7795 - false_positives_6: 560.0000 - true_positives_6: 1415.0000 - false_negatives_6: 1338.0000 - true_negatives_6: 2193.0000 - sensitivity_at_specificity_6: 0.7385 - val_loss: 0.6798 - val_accuracy: 0.6812 - val_recall: 0.5281 - val_precision: 0.7343 - val_auc_6: 0.7807 - val_false_positives_6: 62.0000 - val_true_positives_6: 169.0000 - val_false_negatives_6: 151.0000 - val_true_negatives_6: 258.0000 - val_sensitivity_at_specificity_6: 0.7250\n",
      "Epoch 19/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6804 - accuracy: 0.7025 - recall: 0.5503 - precision: 0.7330 - auc_6: 0.7917 - false_positives_6: 556.0000 - true_positives_6: 1501.0000 - false_negatives_6: 1252.0000 - true_negatives_6: 2197.0000 - sensitivity_at_specificity_6: 0.7588 - val_loss: 0.6789 - val_accuracy: 0.6875 - val_recall: 0.5625 - val_precision: 0.7209 - val_auc_6: 0.7857 - val_false_positives_6: 70.0000 - val_true_positives_6: 180.0000 - val_false_negatives_6: 140.0000 - val_true_negatives_6: 250.0000 - val_sensitivity_at_specificity_6: 0.7375\n",
      "Epoch 20/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6799 - accuracy: 0.6862 - recall: 0.5521 - precision: 0.7164 - auc_6: 0.7839 - false_positives_6: 605.0000 - true_positives_6: 1506.0000 - false_negatives_6: 1247.0000 - true_negatives_6: 2148.0000 - sensitivity_at_specificity_6: 0.7624 - val_loss: 0.6787 - val_accuracy: 0.7000 - val_recall: 0.5844 - val_precision: 0.7382 - val_auc_6: 0.7943 - val_false_positives_6: 67.0000 - val_true_positives_6: 187.0000 - val_false_negatives_6: 133.0000 - val_true_negatives_6: 253.0000 - val_sensitivity_at_specificity_6: 0.7594\n",
      "Epoch 21/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6791 - accuracy: 0.7236 - recall: 0.5991 - precision: 0.7496 - auc_6: 0.8053 - false_positives_6: 556.0000 - true_positives_6: 1637.0000 - false_negatives_6: 1116.0000 - true_negatives_6: 2197.0000 - sensitivity_at_specificity_6: 0.7911 - val_loss: 0.6772 - val_accuracy: 0.7219 - val_recall: 0.6125 - val_precision: 0.7322 - val_auc_6: 0.8039 - val_false_positives_6: 71.0000 - val_true_positives_6: 196.0000 - val_false_negatives_6: 124.0000 - val_true_negatives_6: 249.0000 - val_sensitivity_at_specificity_6: 0.7563\n",
      "Epoch 22/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6786 - accuracy: 0.7029 - recall: 0.5812 - precision: 0.7187 - auc_6: 0.7957 - false_positives_6: 606.0000 - true_positives_6: 1618.0000 - false_negatives_6: 1135.0000 - true_negatives_6: 2147.0000 - sensitivity_at_specificity_6: 0.7792 - val_loss: 0.6768 - val_accuracy: 0.7188 - val_recall: 0.6375 - val_precision: 0.7359 - val_auc_6: 0.8087 - val_false_positives_6: 73.0000 - val_true_positives_6: 204.0000 - val_false_negatives_6: 116.0000 - val_true_negatives_6: 247.0000 - val_sensitivity_at_specificity_6: 0.7750\n",
      "Epoch 23/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6778 - accuracy: 0.7501 - recall: 0.6498 - precision: 0.7805 - auc_6: 0.8303 - false_positives_6: 510.0000 - true_positives_6: 1778.0000 - false_negatives_6: 975.0000 - true_negatives_6: 2243.0000 - sensitivity_at_specificity_6: 0.8195 - val_loss: 0.6762 - val_accuracy: 0.7500 - val_recall: 0.6562 - val_precision: 0.7764 - val_auc_6: 0.8312 - val_false_positives_6: 60.0000 - val_true_positives_6: 210.0000 - val_false_negatives_6: 110.0000 - val_true_negatives_6: 260.0000 - val_sensitivity_at_specificity_6: 0.7906\n",
      "Epoch 24/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6773 - accuracy: 0.7835 - recall: 0.6990 - precision: 0.8204 - auc_6: 0.8579 - false_positives_6: 426.0000 - true_positives_6: 1915.0000 - false_negatives_6: 838.0000 - true_negatives_6: 2327.0000 - sensitivity_at_specificity_6: 0.8503 - val_loss: 0.6757 - val_accuracy: 0.7719 - val_recall: 0.7250 - val_precision: 0.8037 - val_auc_6: 0.8557 - val_false_positives_6: 56.0000 - val_true_positives_6: 232.0000 - val_false_negatives_6: 88.0000 - val_true_negatives_6: 264.0000 - val_sensitivity_at_specificity_6: 0.8281\n",
      "Epoch 25/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6764 - accuracy: 0.7984 - recall: 0.7259 - precision: 0.8275 - auc_6: 0.8680 - false_positives_6: 422.0000 - true_positives_6: 1990.0000 - false_negatives_6: 763.0000 - true_negatives_6: 2331.0000 - sensitivity_at_specificity_6: 0.8631 - val_loss: 0.6745 - val_accuracy: 0.7844 - val_recall: 0.7344 - val_precision: 0.8025 - val_auc_6: 0.8698 - val_false_positives_6: 58.0000 - val_true_positives_6: 235.0000 - val_false_negatives_6: 85.0000 - val_true_negatives_6: 262.0000 - val_sensitivity_at_specificity_6: 0.8438\n",
      "Epoch 26/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6759 - accuracy: 0.7802 - recall: 0.7126 - precision: 0.8003 - auc_6: 0.8539 - false_positives_6: 495.0000 - true_positives_6: 1953.0000 - false_negatives_6: 800.0000 - true_negatives_6: 2258.0000 - sensitivity_at_specificity_6: 0.8463 - val_loss: 0.6744 - val_accuracy: 0.7688 - val_recall: 0.7000 - val_precision: 0.7925 - val_auc_6: 0.8452 - val_false_positives_6: 59.0000 - val_true_positives_6: 224.0000 - val_false_negatives_6: 96.0000 - val_true_negatives_6: 261.0000 - val_sensitivity_at_specificity_6: 0.8125\n",
      "Epoch 27/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6751 - accuracy: 0.7930 - recall: 0.7277 - precision: 0.8069 - auc_6: 0.8692 - false_positives_6: 455.0000 - true_positives_6: 2026.0000 - false_negatives_6: 727.0000 - true_negatives_6: 2298.0000 - sensitivity_at_specificity_6: 0.8692 - val_loss: 0.6736 - val_accuracy: 0.7875 - val_recall: 0.7375 - val_precision: 0.7950 - val_auc_6: 0.8625 - val_false_positives_6: 61.0000 - val_true_positives_6: 236.0000 - val_false_negatives_6: 84.0000 - val_true_negatives_6: 259.0000 - val_sensitivity_at_specificity_6: 0.8313\n",
      "Epoch 28/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6745 - accuracy: 0.8260 - recall: 0.7683 - precision: 0.8337 - auc_6: 0.8925 - false_positives_6: 397.0000 - true_positives_6: 2139.0000 - false_negatives_6: 614.0000 - true_negatives_6: 2356.0000 - sensitivity_at_specificity_6: 0.8874 - val_loss: 0.6725 - val_accuracy: 0.8250 - val_recall: 0.7781 - val_precision: 0.8382 - val_auc_6: 0.8860 - val_false_positives_6: 48.0000 - val_true_positives_6: 249.0000 - val_false_negatives_6: 71.0000 - val_true_negatives_6: 272.0000 - val_sensitivity_at_specificity_6: 0.8594\n",
      "Epoch 29/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6737 - accuracy: 0.8235 - recall: 0.7863 - precision: 0.8424 - auc_6: 0.8954 - false_positives_6: 409.0000 - true_positives_6: 2158.0000 - false_negatives_6: 595.0000 - true_negatives_6: 2344.0000 - sensitivity_at_specificity_6: 0.8878 - val_loss: 0.6712 - val_accuracy: 0.7875 - val_recall: 0.7469 - val_precision: 0.8036 - val_auc_6: 0.8728 - val_false_positives_6: 58.0000 - val_true_positives_6: 239.0000 - val_false_negatives_6: 81.0000 - val_true_negatives_6: 262.0000 - val_sensitivity_at_specificity_6: 0.8313\n",
      "Epoch 30/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6731 - accuracy: 0.8184 - recall: 0.7672 - precision: 0.8244 - auc_6: 0.8895 - false_positives_6: 428.0000 - true_positives_6: 2136.0000 - false_negatives_6: 617.0000 - true_negatives_6: 2325.0000 - sensitivity_at_specificity_6: 0.8834 - val_loss: 0.6702 - val_accuracy: 0.8062 - val_recall: 0.7719 - val_precision: 0.8334 - val_auc_6: 0.8977 - val_false_positives_6: 49.0000 - val_true_positives_6: 247.0000 - val_false_negatives_6: 73.0000 - val_true_negatives_6: 271.0000 - val_sensitivity_at_specificity_6: 0.8594\n",
      "Epoch 31/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6725 - accuracy: 0.8336 - recall: 0.8024 - precision: 0.8461 - auc_6: 0.9035 - false_positives_6: 406.0000 - true_positives_6: 2203.0000 - false_negatives_6: 550.0000 - true_negatives_6: 2347.0000 - sensitivity_at_specificity_6: 0.8928 - val_loss: 0.6702 - val_accuracy: 0.7781 - val_recall: 0.7437 - val_precision: 0.8059 - val_auc_6: 0.8852 - val_false_positives_6: 57.0000 - val_true_positives_6: 238.0000 - val_false_negatives_6: 82.0000 - val_true_negatives_6: 263.0000 - val_sensitivity_at_specificity_6: 0.9750\n",
      "Epoch 32/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6718 - accuracy: 0.8151 - recall: 0.7841 - precision: 0.8337 - auc_6: 0.8925 - false_positives_6: 433.0000 - true_positives_6: 2152.0000 - false_negatives_6: 601.0000 - true_negatives_6: 2320.0000 - sensitivity_at_specificity_6: 0.8812 - val_loss: 0.6694 - val_accuracy: 0.7937 - val_recall: 0.7469 - val_precision: 0.8031 - val_auc_6: 0.8689 - val_false_positives_6: 58.0000 - val_true_positives_6: 239.0000 - val_false_negatives_6: 81.0000 - val_true_negatives_6: 262.0000 - val_sensitivity_at_specificity_6: 0.8188\n",
      "Epoch 33/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6710 - accuracy: 0.8049 - recall: 0.7701 - precision: 0.8157 - auc_6: 0.8822 - false_positives_6: 484.0000 - true_positives_6: 2113.0000 - false_negatives_6: 640.0000 - true_negatives_6: 2269.0000 - sensitivity_at_specificity_6: 0.8696 - val_loss: 0.6680 - val_accuracy: 0.8062 - val_recall: 0.7688 - val_precision: 0.8173 - val_auc_6: 0.8885 - val_false_positives_6: 55.0000 - val_true_positives_6: 246.0000 - val_false_negatives_6: 74.0000 - val_true_negatives_6: 265.0000 - val_sensitivity_at_specificity_6: 0.8562\n",
      "Epoch 34/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6704 - accuracy: 0.8177 - recall: 0.7827 - precision: 0.8185 - auc_6: 0.8928 - false_positives_6: 453.0000 - true_positives_6: 2179.0000 - false_negatives_6: 574.0000 - true_negatives_6: 2300.0000 - sensitivity_at_specificity_6: 0.9666 - val_loss: 0.6677 - val_accuracy: 0.8031 - val_recall: 0.7656 - val_precision: 0.8114 - val_auc_6: 0.8773 - val_false_positives_6: 57.0000 - val_true_positives_6: 245.0000 - val_false_negatives_6: 75.0000 - val_true_negatives_6: 263.0000 - val_sensitivity_at_specificity_6: 0.9656\n",
      "Epoch 35/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6697 - accuracy: 0.8300 - recall: 0.8089 - precision: 0.8404 - auc_6: 0.9013 - false_positives_6: 429.0000 - true_positives_6: 2221.0000 - false_negatives_6: 532.0000 - true_negatives_6: 2324.0000 - sensitivity_at_specificity_6: 0.9666 - val_loss: 0.6670 - val_accuracy: 0.8125 - val_recall: 0.7688 - val_precision: 0.8414 - val_auc_6: 0.8944 - val_false_positives_6: 47.0000 - val_true_positives_6: 246.0000 - val_false_negatives_6: 74.0000 - val_true_negatives_6: 273.0000 - val_sensitivity_at_specificity_6: 0.9688\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "ephocs=35\n",
    "                    \n",
    "\n",
    "steps_per_epoch=train_generator.n//train_generator.batch_size\n",
    "steps_per_epoch\n",
    "validation_steps=validation_generator.n//validation_generator.batch_size\n",
    "validation_steps\n",
    "mod = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=ephocs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_steps\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 136ms/step - loss: 0.6695 - accuracy: 0.8247 - recall: 0.8024 - precision: 0.8418 - auc_6: 0.9009 - false_positives_6: 53.0000 - true_positives_6: 279.0000 - false_negatives_6: 69.0000 - true_negatives_6: 295.0000 - sensitivity_at_specificity_6: 0.9684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6695393323898315,\n",
       " 0.8247126340866089,\n",
       " 0.8023538589477539,\n",
       " 0.8417854309082031,\n",
       " 0.9008950591087341,\n",
       " 53.0,\n",
       " 279.0,\n",
       " 69.0,\n",
       " 295.0,\n",
       " 0.9683908224105835]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = VGG19(input_shape=[100,100]+ [1], weights=None, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(vgg19.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=vgg19.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 100, 100, 1)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      640       \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 9218      \n",
      "=================================================================\n",
      "Total params: 20,032,450\n",
      "Trainable params: 9,218\n",
      "Non-trainable params: 20,023,232\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# view the structure of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "optimum= Adam(learning_rate=0.000005)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimum, metrics=['accuracy',recall,precision,tf.keras.metrics.AUC(curve=\"ROC\"),tf.keras.metrics.FalsePositives(),tf.keras.metrics.TruePositives(),tf.keras.metrics.FalseNegatives(),tf.keras.metrics.TrueNegatives(),tf.keras.metrics.SensitivityAtSpecificity(0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "87/87 [==============================] - 9s 96ms/step - loss: 0.6932 - accuracy: 0.6033 - recall: 0.7640 - precision: 0.6058 - auc_7: 0.4499 - false_positives_7: 1351.0000 - true_positives_7: 2127.0000 - false_negatives_7: 626.0000 - true_negatives_7: 1402.0000 - sensitivity_at_specificity_7: 0.2274 - val_loss: 0.6934 - val_accuracy: 0.5531 - val_recall: 0.8188 - val_precision: 0.5561 - val_auc_7: 0.4191 - val_false_positives_7: 209.0000 - val_true_positives_7: 262.0000 - val_false_negatives_7: 58.0000 - val_true_negatives_7: 111.0000 - val_sensitivity_at_specificity_7: 0.1594\n",
      "Epoch 2/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6929 - accuracy: 0.6306 - recall: 0.9080 - precision: 0.5914 - auc_7: 0.4500 - false_positives_7: 1770.0000 - true_positives_7: 2497.0000 - false_negatives_7: 256.0000 - true_negatives_7: 983.0000 - sensitivity_at_specificity_7: 0.1435 - val_loss: 0.6930 - val_accuracy: 0.5844 - val_recall: 0.9344 - val_precision: 0.5532 - val_auc_7: 0.4607 - val_false_positives_7: 242.0000 - val_true_positives_7: 299.0000 - val_false_negatives_7: 21.0000 - val_true_negatives_7: 78.0000 - val_sensitivity_at_specificity_7: 0.1500\n",
      "Epoch 3/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6926 - accuracy: 0.6589 - recall: 0.9368 - precision: 0.6012 - auc_7: 0.4780 - false_positives_7: 1752.0000 - true_positives_7: 2577.0000 - false_negatives_7: 176.0000 - true_negatives_7: 1001.0000 - sensitivity_at_specificity_7: 0.1576 - val_loss: 0.6926 - val_accuracy: 0.6969 - val_recall: 0.9688 - val_precision: 0.5756 - val_auc_7: 0.4938 - val_false_positives_7: 230.0000 - val_true_positives_7: 310.0000 - val_false_negatives_7: 10.0000 - val_true_negatives_7: 90.0000 - val_sensitivity_at_specificity_7: 0.1562\n",
      "Epoch 4/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6924 - accuracy: 0.7301 - recall: 0.9591 - precision: 0.5778 - auc_7: 0.5196 - false_positives_7: 1929.0000 - true_positives_7: 2639.0000 - false_negatives_7: 114.0000 - true_negatives_7: 824.0000 - sensitivity_at_specificity_7: 0.1725 - val_loss: 0.6922 - val_accuracy: 0.7906 - val_recall: 0.9563 - val_precision: 0.5602 - val_auc_7: 0.5578 - val_false_positives_7: 242.0000 - val_true_positives_7: 306.0000 - val_false_negatives_7: 14.0000 - val_true_negatives_7: 78.0000 - val_sensitivity_at_specificity_7: 0.2125\n",
      "Epoch 5/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6921 - accuracy: 0.7781 - recall: 0.9468 - precision: 0.5578 - auc_7: 0.5769 - false_positives_7: 2115.0000 - true_positives_7: 2605.0000 - false_negatives_7: 148.0000 - true_negatives_7: 638.0000 - sensitivity_at_specificity_7: 0.2107 - val_loss: 0.6918 - val_accuracy: 0.7500 - val_recall: 0.9375 - val_precision: 0.5448 - val_auc_7: 0.6389 - val_false_positives_7: 251.0000 - val_true_positives_7: 300.0000 - val_false_negatives_7: 20.0000 - val_true_negatives_7: 69.0000 - val_sensitivity_at_specificity_7: 0.3187\n",
      "Epoch 6/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6918 - accuracy: 0.6644 - recall: 0.8815 - precision: 0.5410 - auc_7: 0.6420 - false_positives_7: 2104.0000 - true_positives_7: 2423.0000 - false_negatives_7: 330.0000 - true_negatives_7: 649.0000 - sensitivity_at_specificity_7: 0.2913 - val_loss: 0.6916 - val_accuracy: 0.6094 - val_recall: 0.8219 - val_precision: 0.5361 - val_auc_7: 0.6655 - val_false_positives_7: 228.0000 - val_true_positives_7: 263.0000 - val_false_negatives_7: 57.0000 - val_true_negatives_7: 92.0000 - val_sensitivity_at_specificity_7: 0.3281\n",
      "Epoch 7/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6915 - accuracy: 0.5587 - recall: 0.8251 - precision: 0.5459 - auc_7: 0.6698 - false_positives_7: 1926.0000 - true_positives_7: 2266.0000 - false_negatives_7: 487.0000 - true_negatives_7: 827.0000 - sensitivity_at_specificity_7: 0.3313 - val_loss: 0.6910 - val_accuracy: 0.5406 - val_recall: 0.8094 - val_precision: 0.5649 - val_auc_7: 0.7024 - val_false_positives_7: 200.0000 - val_true_positives_7: 259.0000 - val_false_negatives_7: 61.0000 - val_true_negatives_7: 120.0000 - val_sensitivity_at_specificity_7: 0.3969\n",
      "Epoch 8/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6912 - accuracy: 0.4788 - recall: 0.7496 - precision: 0.5383 - auc_7: 0.6832 - false_positives_7: 1800.0000 - true_positives_7: 2056.0000 - false_negatives_7: 697.0000 - true_negatives_7: 953.0000 - sensitivity_at_specificity_7: 0.3578 - val_loss: 0.6906 - val_accuracy: 0.4750 - val_recall: 0.6906 - val_precision: 0.5440 - val_auc_7: 0.7202 - val_false_positives_7: 186.0000 - val_true_positives_7: 221.0000 - val_false_negatives_7: 99.0000 - val_true_negatives_7: 134.0000 - val_sensitivity_at_specificity_7: 0.4250\n",
      "Epoch 9/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6910 - accuracy: 0.4348 - recall: 0.6724 - precision: 0.5177 - auc_7: 0.6934 - false_positives_7: 1712.0000 - true_positives_7: 1841.0000 - false_negatives_7: 912.0000 - true_negatives_7: 1041.0000 - sensitivity_at_specificity_7: 0.3745 - val_loss: 0.6902 - val_accuracy: 0.4781 - val_recall: 0.6687 - val_precision: 0.5433 - val_auc_7: 0.7294 - val_false_positives_7: 180.0000 - val_true_positives_7: 214.0000 - val_false_negatives_7: 106.0000 - val_true_negatives_7: 140.0000 - val_sensitivity_at_specificity_7: 0.4281\n",
      "Epoch 10/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6907 - accuracy: 0.4188 - recall: 0.5995 - precision: 0.4916 - auc_7: 0.7023 - false_positives_7: 1687.0000 - true_positives_7: 1669.0000 - false_negatives_7: 1084.0000 - true_negatives_7: 1066.0000 - sensitivity_at_specificity_7: 0.3836 - val_loss: 0.6897 - val_accuracy: 0.4781 - val_recall: 0.6250 - val_precision: 0.5308 - val_auc_7: 0.7342 - val_false_positives_7: 177.0000 - val_true_positives_7: 200.0000 - val_false_negatives_7: 120.0000 - val_true_negatives_7: 143.0000 - val_sensitivity_at_specificity_7: 0.4375\n",
      "Epoch 11/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6904 - accuracy: 0.4246 - recall: 0.5988 - precision: 0.5020 - auc_7: 0.7049 - false_positives_7: 1657.0000 - true_positives_7: 1636.0000 - false_negatives_7: 1117.0000 - true_negatives_7: 1096.0000 - sensitivity_at_specificity_7: 0.3850 - val_loss: 0.6894 - val_accuracy: 0.4563 - val_recall: 0.5750 - val_precision: 0.5104 - val_auc_7: 0.7251 - val_false_positives_7: 175.0000 - val_true_positives_7: 184.0000 - val_false_negatives_7: 136.0000 - val_true_negatives_7: 145.0000 - val_sensitivity_at_specificity_7: 0.4469\n",
      "Epoch 12/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6901 - accuracy: 0.4185 - recall: 0.5316 - precision: 0.4680 - auc_7: 0.6984 - false_positives_7: 1648.0000 - true_positives_7: 1480.0000 - false_negatives_7: 1273.0000 - true_negatives_7: 1105.0000 - sensitivity_at_specificity_7: 0.3912 - val_loss: 0.6894 - val_accuracy: 0.4469 - val_recall: 0.5281 - val_precision: 0.4857 - val_auc_7: 0.7120 - val_false_positives_7: 179.0000 - val_true_positives_7: 169.0000 - val_false_negatives_7: 151.0000 - val_true_negatives_7: 141.0000 - val_sensitivity_at_specificity_7: 0.4313\n",
      "Epoch 13/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6898 - accuracy: 0.4134 - recall: 0.5108 - precision: 0.4637 - auc_7: 0.6966 - false_positives_7: 1652.0000 - true_positives_7: 1391.0000 - false_negatives_7: 1362.0000 - true_negatives_7: 1101.0000 - sensitivity_at_specificity_7: 0.3916 - val_loss: 0.6889 - val_accuracy: 0.4469 - val_recall: 0.5000 - val_precision: 0.4726 - val_auc_7: 0.7105 - val_false_positives_7: 178.0000 - val_true_positives_7: 160.0000 - val_false_negatives_7: 160.0000 - val_true_negatives_7: 142.0000 - val_sensitivity_at_specificity_7: 0.4375\n",
      "Epoch 14/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6896 - accuracy: 0.4061 - recall: 0.4573 - precision: 0.4296 - auc_7: 0.6788 - false_positives_7: 1655.0000 - true_positives_7: 1273.0000 - false_negatives_7: 1480.0000 - true_negatives_7: 1098.0000 - sensitivity_at_specificity_7: 0.3923 - val_loss: 0.6886 - val_accuracy: 0.4375 - val_recall: 0.4875 - val_precision: 0.4644 - val_auc_7: 0.7023 - val_false_positives_7: 180.0000 - val_true_positives_7: 156.0000 - val_false_negatives_7: 164.0000 - val_true_negatives_7: 140.0000 - val_sensitivity_at_specificity_7: 0.4344\n",
      "Epoch 15/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6894 - accuracy: 0.4072 - recall: 0.4598 - precision: 0.4315 - auc_7: 0.6773 - false_positives_7: 1653.0000 - true_positives_7: 1280.0000 - false_negatives_7: 1473.0000 - true_negatives_7: 1100.0000 - sensitivity_at_specificity_7: 0.3927 - val_loss: 0.6882 - val_accuracy: 0.4531 - val_recall: 0.4969 - val_precision: 0.4728 - val_auc_7: 0.7115 - val_false_positives_7: 176.0000 - val_true_positives_7: 159.0000 - val_false_negatives_7: 161.0000 - val_true_negatives_7: 144.0000 - val_sensitivity_at_specificity_7: 0.4469\n",
      "Epoch 16/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6891 - accuracy: 0.4039 - recall: 0.4468 - precision: 0.4301 - auc_7: 0.6605 - false_positives_7: 1652.0000 - true_positives_7: 1213.0000 - false_negatives_7: 1540.0000 - true_negatives_7: 1101.0000 - sensitivity_at_specificity_7: 0.3952 - val_loss: 0.6878 - val_accuracy: 0.4500 - val_recall: 0.4656 - val_precision: 0.4562 - val_auc_7: 0.6947 - val_false_positives_7: 177.0000 - val_true_positives_7: 149.0000 - val_false_negatives_7: 171.0000 - val_true_negatives_7: 143.0000 - val_sensitivity_at_specificity_7: 0.4437\n",
      "Epoch 17/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6888 - accuracy: 0.4039 - recall: 0.4274 - precision: 0.4135 - auc_7: 0.6568 - false_positives_7: 1654.0000 - true_positives_7: 1190.0000 - false_negatives_7: 1563.0000 - true_negatives_7: 1099.0000 - sensitivity_at_specificity_7: 0.3938 - val_loss: 0.6879 - val_accuracy: 0.4375 - val_recall: 0.4469 - val_precision: 0.4428 - val_auc_7: 0.6876 - val_false_positives_7: 180.0000 - val_true_positives_7: 143.0000 - val_false_negatives_7: 177.0000 - val_true_negatives_7: 140.0000 - val_sensitivity_at_specificity_7: 0.4344\n",
      "Epoch 18/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6885 - accuracy: 0.4046 - recall: 0.4346 - precision: 0.4232 - auc_7: 0.6510 - false_positives_7: 1650.0000 - true_positives_7: 1179.0000 - false_negatives_7: 1574.0000 - true_negatives_7: 1103.0000 - sensitivity_at_specificity_7: 0.3948 - val_loss: 0.6878 - val_accuracy: 0.4281 - val_recall: 0.4406 - val_precision: 0.4357 - val_auc_7: 0.6637 - val_false_positives_7: 183.0000 - val_true_positives_7: 141.0000 - val_false_negatives_7: 179.0000 - val_true_negatives_7: 137.0000 - val_sensitivity_at_specificity_7: 0.4250\n",
      "Epoch 19/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6883 - accuracy: 0.4036 - recall: 0.4303 - precision: 0.4196 - auc_7: 0.6475 - false_positives_7: 1654.0000 - true_positives_7: 1167.0000 - false_negatives_7: 1586.0000 - true_negatives_7: 1099.0000 - sensitivity_at_specificity_7: 0.3934 - val_loss: 0.6868 - val_accuracy: 0.4500 - val_recall: 0.4531 - val_precision: 0.4515 - val_auc_7: 0.6874 - val_false_positives_7: 176.0000 - val_true_positives_7: 145.0000 - val_false_negatives_7: 175.0000 - val_true_negatives_7: 144.0000 - val_sensitivity_at_specificity_7: 0.4469\n",
      "Epoch 20/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6881 - accuracy: 0.4003 - recall: 0.4124 - precision: 0.4050 - auc_7: 0.6387 - false_positives_7: 1655.0000 - true_positives_7: 1148.0000 - false_negatives_7: 1605.0000 - true_negatives_7: 1098.0000 - sensitivity_at_specificity_7: 0.3945 - val_loss: 0.6863 - val_accuracy: 0.4594 - val_recall: 0.4688 - val_precision: 0.4645 - val_auc_7: 0.6966 - val_false_positives_7: 173.0000 - val_true_positives_7: 150.0000 - val_false_negatives_7: 170.0000 - val_true_negatives_7: 147.0000 - val_sensitivity_at_specificity_7: 0.4563\n",
      "Epoch 21/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6877 - accuracy: 0.4043 - recall: 0.4221 - precision: 0.4161 - auc_7: 0.6421 - false_positives_7: 1649.0000 - true_positives_7: 1144.0000 - false_negatives_7: 1609.0000 - true_negatives_7: 1104.0000 - sensitivity_at_specificity_7: 0.3970 - val_loss: 0.6860 - val_accuracy: 0.4437 - val_recall: 0.4500 - val_precision: 0.4476 - val_auc_7: 0.6868 - val_false_positives_7: 178.0000 - val_true_positives_7: 144.0000 - val_false_negatives_7: 176.0000 - val_true_negatives_7: 142.0000 - val_sensitivity_at_specificity_7: 0.4406\n",
      "Epoch 22/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6875 - accuracy: 0.3999 - recall: 0.4156 - precision: 0.4113 - auc_7: 0.6331 - false_positives_7: 1654.0000 - true_positives_7: 1126.0000 - false_negatives_7: 1627.0000 - true_negatives_7: 1099.0000 - sensitivity_at_specificity_7: 0.3974 - val_loss: 0.6860 - val_accuracy: 0.4437 - val_recall: 0.4437 - val_precision: 0.4437 - val_auc_7: 0.6800 - val_false_positives_7: 178.0000 - val_true_positives_7: 142.0000 - val_false_negatives_7: 178.0000 - val_true_negatives_7: 142.0000 - val_sensitivity_at_specificity_7: 0.4406\n",
      "Epoch 23/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6873 - accuracy: 0.3999 - recall: 0.4045 - precision: 0.4001 - auc_7: 0.6300 - false_positives_7: 1656.0000 - true_positives_7: 1126.0000 - false_negatives_7: 1627.0000 - true_negatives_7: 1097.0000 - sensitivity_at_specificity_7: 0.3967 - val_loss: 0.6859 - val_accuracy: 0.4437 - val_recall: 0.4531 - val_precision: 0.4491 - val_auc_7: 0.6771 - val_false_positives_7: 178.0000 - val_true_positives_7: 145.0000 - val_false_negatives_7: 175.0000 - val_true_negatives_7: 142.0000 - val_sensitivity_at_specificity_7: 0.4406\n",
      "Epoch 24/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6870 - accuracy: 0.4025 - recall: 0.4073 - precision: 0.4028 - auc_7: 0.6389 - false_positives_7: 1649.0000 - true_positives_7: 1134.0000 - false_negatives_7: 1619.0000 - true_negatives_7: 1104.0000 - sensitivity_at_specificity_7: 0.3959 - val_loss: 0.6845 - val_accuracy: 0.4656 - val_recall: 0.4719 - val_precision: 0.4692 - val_auc_7: 0.7067 - val_false_positives_7: 171.0000 - val_true_positives_7: 151.0000 - val_false_negatives_7: 169.0000 - val_true_negatives_7: 149.0000 - val_sensitivity_at_specificity_7: 0.4625\n",
      "Epoch 25/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6867 - accuracy: 0.4032 - recall: 0.4080 - precision: 0.4034 - auc_7: 0.6363 - false_positives_7: 1648.0000 - true_positives_7: 1136.0000 - false_negatives_7: 1617.0000 - true_negatives_7: 1105.0000 - sensitivity_at_specificity_7: 0.3967 - val_loss: 0.6845 - val_accuracy: 0.4531 - val_recall: 0.4531 - val_precision: 0.4531 - val_auc_7: 0.6993 - val_false_positives_7: 175.0000 - val_true_positives_7: 145.0000 - val_false_negatives_7: 175.0000 - val_true_negatives_7: 145.0000 - val_sensitivity_at_specificity_7: 0.4500\n",
      "Epoch 26/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6866 - accuracy: 0.4025 - recall: 0.4088 - precision: 0.4032 - auc_7: 0.6383 - false_positives_7: 1652.0000 - true_positives_7: 1138.0000 - false_negatives_7: 1615.0000 - true_negatives_7: 1101.0000 - sensitivity_at_specificity_7: 0.3952 - val_loss: 0.6850 - val_accuracy: 0.4406 - val_recall: 0.4469 - val_precision: 0.4443 - val_auc_7: 0.6780 - val_false_positives_7: 179.0000 - val_true_positives_7: 143.0000 - val_false_negatives_7: 177.0000 - val_true_negatives_7: 141.0000 - val_sensitivity_at_specificity_7: 0.4344\n",
      "Epoch 27/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6863 - accuracy: 0.4097 - recall: 0.4152 - precision: 0.4087 - auc_7: 0.6448 - false_positives_7: 1642.0000 - true_positives_7: 1156.0000 - false_negatives_7: 1597.0000 - true_negatives_7: 1111.0000 - sensitivity_at_specificity_7: 0.3959 - val_loss: 0.6842 - val_accuracy: 0.4594 - val_recall: 0.4688 - val_precision: 0.4635 - val_auc_7: 0.6998 - val_false_positives_7: 174.0000 - val_true_positives_7: 150.0000 - val_false_negatives_7: 170.0000 - val_true_negatives_7: 146.0000 - val_sensitivity_at_specificity_7: 0.4500\n",
      "Epoch 28/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6861 - accuracy: 0.4108 - recall: 0.4138 - precision: 0.4077 - auc_7: 0.6451 - false_positives_7: 1642.0000 - true_positives_7: 1152.0000 - false_negatives_7: 1601.0000 - true_negatives_7: 1111.0000 - sensitivity_at_specificity_7: 0.3938 - val_loss: 0.6837 - val_accuracy: 0.4656 - val_recall: 0.4688 - val_precision: 0.4670 - val_auc_7: 0.7059 - val_false_positives_7: 172.0000 - val_true_positives_7: 150.0000 - val_false_negatives_7: 170.0000 - val_true_negatives_7: 148.0000 - val_sensitivity_at_specificity_7: 0.4531\n",
      "Epoch 29/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6858 - accuracy: 0.4116 - recall: 0.4282 - precision: 0.4222 - auc_7: 0.6461 - false_positives_7: 1631.0000 - true_positives_7: 1161.0000 - false_negatives_7: 1592.0000 - true_negatives_7: 1122.0000 - sensitivity_at_specificity_7: 0.3959 - val_loss: 0.6837 - val_accuracy: 0.4531 - val_recall: 0.4563 - val_precision: 0.4548 - val_auc_7: 0.6974 - val_false_positives_7: 175.0000 - val_true_positives_7: 146.0000 - val_false_negatives_7: 174.0000 - val_true_negatives_7: 145.0000 - val_sensitivity_at_specificity_7: 0.4500\n",
      "Epoch 30/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6854 - accuracy: 0.4116 - recall: 0.4170 - precision: 0.4109 - auc_7: 0.6434 - false_positives_7: 1632.0000 - true_positives_7: 1161.0000 - false_negatives_7: 1592.0000 - true_negatives_7: 1121.0000 - sensitivity_at_specificity_7: 0.3981 - val_loss: 0.6838 - val_accuracy: 0.4437 - val_recall: 0.4500 - val_precision: 0.4476 - val_auc_7: 0.6868 - val_false_positives_7: 178.0000 - val_true_positives_7: 144.0000 - val_false_negatives_7: 176.0000 - val_true_negatives_7: 142.0000 - val_sensitivity_at_specificity_7: 0.4375\n",
      "Epoch 31/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6853 - accuracy: 0.4090 - recall: 0.4235 - precision: 0.4187 - auc_7: 0.6393 - false_positives_7: 1637.0000 - true_positives_7: 1148.0000 - false_negatives_7: 1605.0000 - true_negatives_7: 1116.0000 - sensitivity_at_specificity_7: 0.3963 - val_loss: 0.6832 - val_accuracy: 0.4469 - val_recall: 0.4563 - val_precision: 0.4520 - val_auc_7: 0.6901 - val_false_positives_7: 177.0000 - val_true_positives_7: 146.0000 - val_false_negatives_7: 174.0000 - val_true_negatives_7: 143.0000 - val_sensitivity_at_specificity_7: 0.4406\n",
      "Epoch 32/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6849 - accuracy: 0.4108 - recall: 0.4217 - precision: 0.4177 - auc_7: 0.6385 - false_positives_7: 1635.0000 - true_positives_7: 1143.0000 - false_negatives_7: 1610.0000 - true_negatives_7: 1118.0000 - sensitivity_at_specificity_7: 0.3959 - val_loss: 0.6829 - val_accuracy: 0.4500 - val_recall: 0.4531 - val_precision: 0.4520 - val_auc_7: 0.6879 - val_false_positives_7: 176.0000 - val_true_positives_7: 145.0000 - val_false_negatives_7: 175.0000 - val_true_negatives_7: 144.0000 - val_sensitivity_at_specificity_7: 0.4437\n",
      "Epoch 33/35\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.6848 - accuracy: 0.4065 - recall: 0.4084 - precision: 0.4050 - auc_7: 0.6368 - false_positives_7: 1639.0000 - true_positives_7: 1137.0000 - false_negatives_7: 1616.0000 - true_negatives_7: 1114.0000 - sensitivity_at_specificity_7: 0.3967 - val_loss: 0.6821 - val_accuracy: 0.4563 - val_recall: 0.4594 - val_precision: 0.4563 - val_auc_7: 0.6955 - val_false_positives_7: 175.0000 - val_true_positives_7: 147.0000 - val_false_negatives_7: 173.0000 - val_true_negatives_7: 145.0000 - val_sensitivity_at_specificity_7: 0.4500\n",
      "Epoch 34/35\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.6845 - accuracy: 0.4097 - recall: 0.4113 - precision: 0.4079 - auc_7: 0.6397 - false_positives_7: 1629.0000 - true_positives_7: 1145.0000 - false_negatives_7: 1608.0000 - true_negatives_7: 1124.0000 - sensitivity_at_specificity_7: 0.3963 - val_loss: 0.6824 - val_accuracy: 0.4469 - val_recall: 0.4500 - val_precision: 0.4480 - val_auc_7: 0.6850 - val_false_positives_7: 177.0000 - val_true_positives_7: 144.0000 - val_false_negatives_7: 176.0000 - val_true_negatives_7: 143.0000 - val_sensitivity_at_specificity_7: 0.4375\n",
      "Epoch 35/35\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.6843 - accuracy: 0.4145 - recall: 0.4256 - precision: 0.4212 - auc_7: 0.6390 - false_positives_7: 1627.0000 - true_positives_7: 1154.0000 - false_negatives_7: 1599.0000 - true_negatives_7: 1126.0000 - sensitivity_at_specificity_7: 0.3970 - val_loss: 0.6816 - val_accuracy: 0.4625 - val_recall: 0.4656 - val_precision: 0.4629 - val_auc_7: 0.7012 - val_false_positives_7: 173.0000 - val_true_positives_7: 149.0000 - val_false_negatives_7: 171.0000 - val_true_negatives_7: 147.0000 - val_sensitivity_at_specificity_7: 0.4594\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "ephocs=35\n",
    "                    \n",
    "\n",
    "steps_per_epoch=train_generator.n//train_generator.batch_size\n",
    "steps_per_epoch\n",
    "validation_steps=validation_generator.n//validation_generator.batch_size\n",
    "validation_steps\n",
    "mod = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=ephocs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_steps\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 78ms/step - loss: 0.6841 - accuracy: 0.4138 - recall: 0.4205 - precision: 0.4180 - auc_7: 0.6408 - false_positives_7: 204.0000 - true_positives_7: 146.0000 - false_negatives_7: 202.0000 - true_negatives_7: 144.0000 - sensitivity_at_specificity_7: 0.4023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6841331720352173,\n",
       " 0.4137931168079376,\n",
       " 0.4204545319080353,\n",
       " 0.4180440902709961,\n",
       " 0.6408458948135376,\n",
       " 204.0,\n",
       " 146.0,\n",
       " 202.0,\n",
       " 144.0,\n",
       " 0.40229883790016174]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator,verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_tf",
   "language": "python",
   "name": "gpu_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
